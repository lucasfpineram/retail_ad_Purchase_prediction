{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, KFold, cross_val_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures, StandardScaler, OneHotEncoder\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------\n",
    "# Intento 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATA y drop atribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_id\n",
       "14540    717\n",
       "11932    701\n",
       "27372    622\n",
       "14900    535\n",
       "12183    517\n",
       "        ... \n",
       "23949      1\n",
       "3863       1\n",
       "14492      1\n",
       "18426      1\n",
       "11527      1\n",
       "Name: count, Length: 27695, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_comp():\n",
    "    # Load the competition data\n",
    "    comp_data = pd.read_csv(\"../competition_data.csv\")\n",
    "    comp_data = comp_data.drop(\n",
    "        columns=[\n",
    "            \"benefit\", \n",
    "            \"etl_version\", \n",
    "            \"uid\",\n",
    "            \"date\",\n",
    "            \"deal_print_id\",\n",
    "            \"full_name\",\n",
    "            \"main_picture\",\n",
    "            \"warranty\",\n",
    "            \"tags\",\n",
    "            \"print_server_timestamp\",\n",
    "            \"print_position\",\n",
    "            \"title\",\n",
    "            \"accepts_mercadopago\"\n",
    "        ]\n",
    "    )\n",
    "    # OHE comp_data columns\n",
    "    comp_data = pd.get_dummies(comp_data, \n",
    "        sparse=True,\n",
    "        columns=[\n",
    "            #\"category_id\", \n",
    "            #\"domain_id\", \n",
    "            \"logistic_type\", \n",
    "            \"platform\", \n",
    "            \"site_id\"\n",
    "        ],\n",
    "        dtype=int\n",
    "    )\n",
    "    #comp_data[\"accepts_mercadopago\"] = comp_data[\"accepts_mercadopago\"].astype(int)\n",
    "    # todos aceptan mercadopago\n",
    "    comp_data[\"boosted\"] = comp_data[\"boosted\"].astype(int)\n",
    "    comp_data[\"free_shipping\"] = comp_data[\"free_shipping\"].astype(int)\n",
    "    comp_data[\"fulfillment\"] = comp_data[\"fulfillment\"].astype(int)\n",
    "\n",
    "    comp_data[\"is_pdp\"].fillna(0, inplace=True)\n",
    "    comp_data[\"is_pdp\"] = comp_data[\"is_pdp\"].astype(int)\n",
    "    #comp_data[\"warranty\"] = comp_data[\"warranty\"].astype(int)\n",
    "\n",
    "    # comp_data[\"listing_type_id\"] to 0 if gold_special, 1 if gold_pro.\n",
    "    comp_data[\"listing_type_id\"] = comp_data[\"listing_type_id\"].apply(lambda x: 0 if x == \"gold_special\" else 1)\n",
    "    \n",
    "    # Label encode category_id and domain_id\n",
    "    # comp_data[\"category_id\"] = comp_data[\"category_id\"].astype(\"category\")#.cat.codes\n",
    "    #comp_data[\"domain_id\"] = comp_data[\"domain_id\"].astype(\"category\")#.cat.codes\n",
    "\n",
    "    # sklearn LabelEncoder for category_id and domain_id\n",
    "    \n",
    "    comp_data[\"category_id\"] = LabelEncoder().fit_transform(comp_data[\"category_id\"]).astype(int)\n",
    "    comp_data[\"domain_id\"] = LabelEncoder().fit_transform(comp_data[\"domain_id\"]).astype(int)\n",
    "    comp_data[\"item_id\"] = LabelEncoder().fit_transform(comp_data[\"item_id\"]).astype(int)\n",
    "\n",
    "    comp_data[\"price_diff\"] = comp_data[\"price\"] - comp_data[\"original_price\"]\n",
    "    comp_data[\"cheaper_than_original\"] = comp_data[\"price_diff\"].apply(lambda x: 1 if x < 0 else 0)\n",
    "    comp_data[\"price_diff\"] = comp_data[\"price_diff\"].apply(lambda x: abs(x)).astype(int)\n",
    "\n",
    "    #comp_data[\"cheaper_than_avg\"] = comp_data[\"price\"] - comp_data[\"avg_asp_item_domain\"]\n",
    "    #comp_data[\"cheaper_than_avg\"] = comp_data[\"cheaper_than_avg\"].apply(lambda x: 1 if x < 0 else 0).astype(int)\n",
    "\n",
    "    # Drop useless columns\n",
    "    # comp_data = comp_data.drop(\n",
    "    #     columns=[\n",
    "    #     ]\n",
    "    # )\n",
    "\n",
    "    return comp_data\n",
    "comp_data = load_comp()\n",
    "comp_data[\"item_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLIT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **available_quantity** (int64)\n",
       "- **avg_gmv_item_domain_30days** (float64)\n",
       "- **avg_gmv_item_sel** (float64)\n",
       "- **avg_gmv_seller_bday** (float64)\n",
       "- **avg_qty_orders_item_domain_30days** (float64)\n",
       "- **avg_qty_orders_item_sel_30days** (float64)\n",
       "- **avg_si_item_sel_30day** (float64)\n",
       "- **boosted** (int32)\n",
       "- **category_id** (int32)\n",
       "- **conversion** (float64)\n",
       "- **domain_id** (int32)\n",
       "- **free_shipping** (int32)\n",
       "- **fulfillment** (int32)\n",
       "- **health** (float64)\n",
       "- **is_pdp** (int32)\n",
       "- **product_id** (float64)\n",
       "- **item_id** (int32)\n",
       "- **listing_type_id** (int64)\n",
       "- **offset** (int64)\n",
       "- **original_price** (int64)\n",
       "- **price** (int64)\n",
       "- **qty_items_dom** (float64)\n",
       "- **qty_items_sel** (float64)\n",
       "- **sold_quantity** (int64)\n",
       "- **total_asp_item_domain_30days** (float64)\n",
       "- **total_asp_item_sel_30days** (float64)\n",
       "- **total_gmv_domain_bday** (float64)\n",
       "- **total_gmv_item_30days** (float64)\n",
       "- **total_items_domain** (int64)\n",
       "- **total_items_seller** (int64)\n",
       "- **total_orders_domain_30days** (float64)\n",
       "- **total_orders_item_30days** (float64)\n",
       "- **total_orders_sel_30days** (float64)\n",
       "- **total_si_domain_30days** (float64)\n",
       "- **total_si_item_30days** (float64)\n",
       "- **total_si_sel_30days** (float64)\n",
       "- **total_visits_domain** (int64)\n",
       "- **total_visits_item** (int64)\n",
       "- **total_visits_seller** (int64)\n",
       "- **user_id** (float64)\n",
       "- **ROW_ID** (float64)\n",
       "- **logistic_type_cross_docking** (Sparse[int32, 0])\n",
       "- **logistic_type_custom** (Sparse[int32, 0])\n",
       "- **logistic_type_default** (Sparse[int32, 0])\n",
       "- **logistic_type_drop_off** (Sparse[int32, 0])\n",
       "- **logistic_type_fulfillment** (Sparse[int32, 0])\n",
       "- **logistic_type_not_specified** (Sparse[int32, 0])\n",
       "- **logistic_type_xd_drop_off** (Sparse[int32, 0])\n",
       "- **platform_/mobile/android** (Sparse[int32, 0])\n",
       "- **platform_/mobile/ios** (Sparse[int32, 0])\n",
       "- **platform_/web/desktop** (Sparse[int32, 0])\n",
       "- **platform_/web/mobile** (Sparse[int32, 0])\n",
       "- **site_id_MLA** (Sparse[int32, 0])\n",
       "- **price_diff** (int32)\n",
       "- **cheaper_than_original** (int64)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into training and evaluation samples\n",
    "comp_data = load_comp()\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        \"\\n\".join(\n",
    "            [\n",
    "                \"- **{}** ({})\".format(col, dtype) for col, dtype in\n",
    "                zip(comp_data.columns, comp_data.dtypes)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "full_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "\n",
    "# Get number column names for full_data\n",
    "#print(full_data.columns)\n",
    "#num_cols = full_data.select_dtypes(include='number').columns\n",
    "#print(num_cols)\n",
    "# difference between full_data_cols and num_cols\n",
    "# print(set(full_data.columns) - set(num_cols))\n",
    "\n",
    "del comp_data\n",
    "gc.collect()\n",
    "\n",
    "train_data, test_data = train_test_split(full_data, test_size=0.20, train_size=0.80, random_state=42)\n",
    "\n",
    "y_train = train_data[\"conversion\"]\n",
    "X_train = train_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "X_train = X_train.select_dtypes(include='number')\n",
    "\n",
    "y_test = test_data[\"conversion\"]\n",
    "X_test = test_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "X_test = X_test.select_dtypes(include='number')\n",
    "\n",
    "del train_data\n",
    "del test_data\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC split 1:  0.7116231505769222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC split 2:  0.7235289611958904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC split 3:  0.7236594810379241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC split 4:  0.6869342415878192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC split 5:  0.7223620781213635\n",
      "ROC-AUC mean:  0.7136215825039838\n",
      "ROC-AUC:  0.7246614377882451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "atributes_to_poly = [\n",
    "                        \"available_quantity\", \n",
    "                        \"avg_gmv_item_domain_30days\", \n",
    "                        \"avg_gmv_item_sel\", \n",
    "                        \"avg_gmv_seller_bday\", \n",
    "                        \"avg_qty_orders_item_domain_30days\", \n",
    "                        \"avg_qty_orders_item_sel_30days\", \n",
    "                        \"avg_si_item_sel_30day\",\n",
    "                        \"original_price\",\n",
    "                        \"price\",\n",
    "                    ]\n",
    "\n",
    "imputed_poly_features = make_pipeline(\n",
    "  #SimpleImputer(),\n",
    "  #PolynomialFeatures(),\n",
    "  StandardScaler(),\n",
    "  xgb.XGBClassifier(\n",
    "        missing=np.nan,\n",
    "        n_jobs=-1,\n",
    "        tree_method='gpu_hist',\n",
    "    )\n",
    ")\n",
    "\n",
    "# Custom Layer Starts Here:\n",
    "pl = PolynomialFeatures(2, interaction_only=True, include_bias=False)\n",
    "\n",
    "#Input X contains NaN.\n",
    "\n",
    "imputed_X_train_plus = X_train.copy()\n",
    "imputed_X_test_plus = X_test.copy()\n",
    "print(imputed_X_train_plus.shape[0] == X_train.shape[0])\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "cols_with_missing = (col for col in X_train.columns\n",
    "                        if X_train[col].isnull().any())\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n",
    "    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "poly_X_train = imputed_X_train_plus[atributes_to_poly]\n",
    "poly_X_test = imputed_X_test_plus[atributes_to_poly]\n",
    "print(poly_X_train.shape[0] == X_train.shape[0])\n",
    "\n",
    "poly_X_train = my_imputer.fit_transform(poly_X_train)\n",
    "poly_X_test = my_imputer.transform(poly_X_test)\n",
    "print(poly_X_train.shape[0] == X_train.shape[0])\n",
    "\n",
    "poly_X_train = pl.fit_transform(poly_X_train)\n",
    "poly_X_test = pl.fit_transform(poly_X_test)\n",
    "print(poly_X_train.shape[0] == X_train.shape[0])\n",
    "\n",
    "new_X_train = np.concatenate((X_train.drop(atributes_to_poly, axis=1), poly_X_train), axis=1)\n",
    "new_X_test = np.concatenate((X_test.drop(atributes_to_poly, axis=1), poly_X_test), axis=1)\n",
    "print(new_X_train.shape[0] == X_train.shape[0])\n",
    "# Custom Layer Ends Here\n",
    "\n",
    "\n",
    "imputed_poly_features.fit(new_X_train, y_train)\n",
    "\n",
    "roc_auc_score(y_test, imputed_poly_features.predict_proba(new_X_test)[:, imputed_poly_features.classes_ == 1])\n",
    "# Decision Tree Classifier with K-Fold Cross Validation and Randomized Search CV\n",
    "\n",
    "dtc = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    RandomizedSearchCV(\n",
    "        DecisionTreeClassifier(random_state=2345),\n",
    "        param_distributions={\n",
    "            \"max_depth\": [2, 4, 8, 16, 32, 64, 128, 256, 512, None],\n",
    "            \"min_samples_split\": [2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "            \"min_samples_leaf\": [2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "            \"max_features\": [2, 4, 8, 16, 32, 64, 128, 256, 512, None],\n",
    "        },\n",
    "        n_iter=100,\n",
    "        cv=3,\n",
    "        random_state=2345,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=2345, shuffle=True)\n",
    "roc_aucs = []\n",
    "\n",
    "for train_index, valid_index in kf.split(X_train):\n",
    "    X_train_kf, X_valid_kf = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "    y_train_kf, y_valid_kf = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "\n",
    "    dtc.fit(X_train_kf, y_train_kf)\n",
    "    # Metric: roc_auc_score\n",
    "    roc_aucs.append(roc_auc_score(y_valid_kf, dtc.predict_proba(X_valid_kf)[:, dtc.classes_ == 1]))\n",
    "    print(f\"ROC-AUC split {len(roc_aucs)}: \", roc_aucs[-1])\n",
    "\n",
    "print(\"ROC-AUC mean: \", sum(roc_aucs) / len(roc_aucs))    \n",
    "\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "print(\"ROC-AUC: \", roc_auc_score(y_test, dtc.predict_proba(X_test)[:, dtc.classes_ == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"ROW_ID\": eval_data[\"ROW_ID\"], \"conversion\": y_preds})\n",
    "submission_df[\"ROW_ID\"] = submission_df[\"ROW_ID\"].astype(int)\n",
    "submission_df.to_csv(\"exp_oli_1.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------\n",
    "# Intento 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFOLD CLASE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross validation\n",
    "kcv = KFold(n_splits=10, shuffle=True)\n",
    "rmse_kcv1 = []\n",
    "for train_ix, test_ix in tqdm(kcv.split(X_train), total=10):\n",
    "    X_train_red, X_val = X_train.iloc[train_ix,:], X_train.iloc[test_ix,:]\n",
    "    y_train_red, y_val = y_train[train_ix], y_train[test_ix]\n",
    "    tree = DecisionTreeRegressor(max_depth=10, random_state=TREE_SEED)\n",
    "    tree.fit(X_train_red, y_train_red)\n",
    "    preds_val = tree.predict(X_val)\n",
    "    rmse_kcv1.append(math.sqrt(mean_squared_error(y_val, preds_val)))\n",
    "\n",
    "print(f\"RMSE estimado mediante k-fold CV (Versión 1): {mean(rmse_kcv1):.2f}\")\n",
    "\n",
    "# Validación cruzada k-fold utilizando cross_val_score\n",
    "kcv = KFold(n_splits=10, shuffle=True)\n",
    "tree = DecisionTreeRegressor(max_depth=10, random_state=TREE_SEED)\n",
    "rmse_kcv2 = cross_val_score(tree, X_train, y_train, scoring='neg_root_mean_squared_error', cv=kcv, n_jobs=-1)\n",
    "\n",
    "print(f\"RMSE estimado mediante k-fold CV (Versión 2): {-1 * mean(rmse_kcv2):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "no se si usar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, holdout y testing sets\n",
    "size_val = math.ceil(0.1 * X_train.shape[0])\n",
    "size_test = math.ceil(0.1 * X_train.shape[0])\n",
    "\n",
    "X_train_red, X_test, y_train_red, y_test = train_test_split(X_train, y_train, test_size=size_test)\n",
    "X_train_red, X_val, y_train_red, y_val = train_test_split(X_train_red, y_train_red, test_size=size_val)\n",
    "\n",
    "# Prueba con distintos niveles de profundidad (model selection)\n",
    "exp_results = []\n",
    "for md in tqdm(range(1, 51)):\n",
    "    tree = DecisionTreeRegressor(max_depth=md, random_state=TREE_SEED)\n",
    "    tree.fit(X_train_red, y_train_red)\n",
    "    preds_val = tree.predict(X_val)\n",
    "    exp_results.append({\"max_depth\": md,\n",
    "                        \"rmse_val\": math.sqrt(mean_squared_error(y_val, preds_val))})\n",
    "\n",
    "exp_results = pd.DataFrame(exp_results)\n",
    "plot_exp(exp_results)\n",
    "\n",
    "# Se entrena el árbol con la mejor profundidad encontrada sobre train set + validation set\n",
    "best_md = exp_results[exp_results[\"rmse_val\"].min() == exp_results[\"rmse_val\"]]\n",
    "best_md = best_md.sort_values(\"max_depth\").iloc[0,:]\n",
    "print(f\"Performance del mejor modelos: {best_md['rmse_val']:.2f}\")\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=int(best_md[\"max_depth\"]), random_state=TREE_SEED)\n",
    "tree.fit(pd.concat([X_train_red, X_val], axis=0),\n",
    "         pd.concat([y_train_red, y_val], axis=0))\n",
    "preds_test = tree.predict(X_test)\n",
    "\n",
    "print(f\"RMSE estimado en test: {math.sqrt(mean_squared_error(y_test, preds_test)):.2f}\")\n",
    "\n",
    "# Evaluación final en el conjunto de evaluación (model assestment)\n",
    "tree = DecisionTreeRegressor(max_depth=int(best_md[\"max_depth\"]), random_state=TREE_SEED)\n",
    "tree.fit(X_train, y_train)\n",
    "preds_eval = tree.predict(X_eval)\n",
    "_, _, _, y_eval = load_data()\n",
    "\n",
    "print(f\"Performance de evaluación: {math.sqrt(mean_squared_error(y_eval, preds_eval)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot-encoding\n",
    "# Opción 1: utilizar get_dummies de pandas\n",
    "X_train_ohe_v1 = pd.get_dummies(X_train).head()  # Y ahora cómo lo aplico en validación? (debería unir antes los datos)\n",
    "\n",
    "# Opción 2: utilizar onehotencoder de sklearn (más conveniente para producción)\n",
    "\n",
    "# Sobre training\n",
    "X_train_obj = X_train.select_dtypes(include=[\"object\"])\n",
    "X_train_num = X_train.select_dtypes(include=[\"int\", \"float\"])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder(sparse_output=True)\n",
    "X_train_ohe_obj = encoder.fit_transform(X_train_obj)\n",
    "X_train_ohe_v2 = hstack([X_train_ohe_obj, scaler.fit_transform(X_train_num)])\n",
    "\n",
    "# Sobre validation\n",
    "X_val_obj = X_val.select_dtypes(include=[\"object\"])\n",
    "X_val_num = X_val.select_dtypes(include=[\"int\", \"float\"])\n",
    "\n",
    "X_val_ohe_obj = encoder.transform(X_val_obj)\n",
    "X_val_ohe_v2 = hstack([X_val_ohe_obj, scaler.transform(X_val_num)])\n",
    "\n",
    "# Entrenamos un modelo de regresión logística sobre los datos con OHE sin escalado\n",
    "lr_exp = []\n",
    "for C in tqdm([0.001, 0.0025, 0.01, 0.05, 0.075, 0.1, 0.25]):\n",
    "    lr = LogisticRegression(C=C, max_iter=2000)\n",
    "    lr.fit(X_train_ohe_v2, y_train)\n",
    "    preds = lr.predict_proba(X_val_ohe_v2)\n",
    "    preds = preds[:, lr.classes_ == \"yes\"]\n",
    "    auc = roc_auc_score(y_val == \"yes\", preds)\n",
    "    lr_exp.append({\"C\": C, \"auc_val\": auc})\n",
    "\n",
    "lr_exp = pd.DataFrame(lr_exp)\n",
    "print(lr_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [180761, 19211]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\oltie\\OneDrive\\Escritorio\\DiTella\\6to_Semestre_23\\TD6\\TP2\\tp2_td6\\experimentacion.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m eval_data \u001b[39m=\u001b[39m comp_data[comp_data[\u001b[39m\"\u001b[39m\u001b[39mROW_ID\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mnotna()]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m tree \u001b[39m=\u001b[39m DecisionTreeClassifier(random_state \u001b[39m=\u001b[39m \u001b[39m22\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m scores \u001b[39m=\u001b[39m cross_val_score(tree, train_data, eval_data, cv \u001b[39m=\u001b[39;49m KFold(\u001b[39m4\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdel\u001b[39;00m comp_data\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    560\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 562\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    563\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    564\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    565\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    566\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    567\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    568\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    569\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    570\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    571\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    572\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    573\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    574\u001b[0m )\n\u001b[0;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:290\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[0;32m     56\u001b[0m     {\n\u001b[0;32m     57\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m: [HasMethods(\u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m)],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m     error_score\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mnan,\n\u001b[0;32m     97\u001b[0m ):\n\u001b[0;32m     98\u001b[0m     \u001b[39m\"\"\"Evaluate metric(s) by cross-validation and also record fit/score times.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \n\u001b[0;32m    100\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <multimetric_cross_validation>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39m    [0.28009951 0.3908844  0.22784907]\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m     X, y, groups \u001b[39m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m    292\u001b[0m     cv \u001b[39m=\u001b[39m check_cv(cv, y, classifier\u001b[39m=\u001b[39mis_classifier(estimator))\n\u001b[0;32m    294\u001b[0m     \u001b[39mif\u001b[39;00m callable(scoring):\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:455\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \n\u001b[0;32m    438\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    454\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[1;32m--> 455\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[0;32m    456\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    407\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    412\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [180761, 19211]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the competition data\n",
    "comp_data = pd.read_csv(\"../competition_data.csv\")\n",
    "\n",
    "# Split into training and evaluation samples\n",
    "# dividimos el DataFrame original comp_data en dos conjuntos: uno (train_data) que contiene filas con \n",
    "# valores nulos en la columna \"ROW_ID\" y otro (eval_data) que contiene filas con valores no nulos \n",
    "# en la misma columna.\n",
    "train_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "tree = DecisionTreeClassifier(random_state = 22)\n",
    "scores = cross_val_score(tree, train_data, eval_data, cv = KFold(4))\n",
    "del comp_data\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "###\n",
    "parameters = {'criterion':('gini', 'entropy', 'log_loss'),\n",
    "              'splitter': ('best', 'random'),\n",
    "              'max_depth': list(range(5, 41)),\n",
    "              'min_samples_split': list(range(2, 21)),\n",
    "              'min_samples_leaf': list(range(1, 16)),\n",
    "              'min_impurity_decrease': uniform(loc = 0, scale = 0.1) \n",
    "             }\n",
    "\n",
    "rs = RandomizedSearchCV(estimator = DecisionTreeClassifier(random_state = 22),\n",
    "                        param_distributions = parameters,\n",
    "                        n_iter = 1800, # cantidad de iteraciones, sino haria infinitas\n",
    "                        cv = KFold(4),\n",
    "                        random_state = 22)\n",
    "\n",
    "rs.fit(train_data, eval_data)\n",
    "###\n",
    "\n",
    "\n",
    "# Train a random forest model on the train data\n",
    "train_data = train_data.sample(frac=1/3) # toma un tercio de los datos \n",
    "y_train = train_data[\"conversion\"]\n",
    "X_train = train_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "X_train = X_train.select_dtypes(include='number') # solo columnas numericas\n",
    "del train_data\n",
    "gc.collect()\n",
    "\n",
    "cls = make_pipeline(SimpleImputer(), DecisionTreeClassifier(max_depth=8, random_state=2345))\n",
    "cls.fit(X_train, y_train)\n",
    "# modelo del árbol de decisión se entrenará en las características de X_train y aprenderá a \n",
    "# predecir las etiquetas de y_train utilizando los valores faltantes imputados por el SimpleImputer.\n",
    "\n",
    "# Predict on the evaluation set\n",
    "eval_data = eval_data.drop(columns=[\"conversion\"]) # elimina la columna conversion\n",
    "eval_data = eval_data.select_dtypes(include='number') # filtra y deja solo los valores numericos\n",
    "y_preds = cls.predict_proba(eval_data.drop(columns=[\"ROW_ID\"]))[:, cls.classes_ == 1].squeeze() # utiliza el modelo entrenado para hacer las predicciones\n",
    "\n",
    "# Make the submission file\n",
    "submission_df = pd.DataFrame({\"ROW_ID\": eval_data[\"ROW_ID\"], \"conversion\": y_preds})\n",
    "submission_df[\"ROW_ID\"] = submission_df[\"ROW_ID\"].astype(int)\n",
    "submission_df.to_csv(\"basic_model.csv\", sep=\",\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

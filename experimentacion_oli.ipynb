{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, KFold, cross_val_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures, StandardScaler, OneHotEncoder\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 accepts_mercadopago 1\n",
      "8 benefit 3\n",
      "9 boosted 1\n",
      "11 conversion 3\n",
      "12 date 31\n",
      "15 etl_version 1\n",
      "16 free_shipping 2\n",
      "17 fulfillment 2\n",
      "19 health 35\n",
      "20 is_pdp 3\n",
      "23 listing_type_id 2\n",
      "24 logistic_type 7\n",
      "28 platform 4\n",
      "34 site_id 1\n",
      "199972\n",
      "['accepts_mercadopago', 'benefit', 'boosted', 'conversion', 'date', 'etl_version', 'free_shipping', 'fulfillment', 'health', 'is_pdp', 'listing_type_id', 'logistic_type', 'platform', 'site_id']\n"
     ]
    }
   ],
   "source": [
    "menores_a_50 = []\n",
    "comp_data = pd.read_csv(\"./competition_data.csv\")\n",
    "for i in range(len(comp_data.columns)):\n",
    "    if (len(comp_data[comp_data.columns[i]].unique()) < 50):\n",
    "        menores_a_50.append(comp_data.columns[i])\n",
    "        print(i,comp_data.columns[i], len(comp_data[comp_data.columns[i]].unique()))\n",
    "print(len(comp_data))\n",
    "print(menores_a_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data.describe(include = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'comp_data' is your DataFrame\n",
    "data = []\n",
    "\n",
    "for i, column_name in enumerate(comp_data.columns):\n",
    "    # if isinstance(comp_data[column_name][0], (float, int)) and column_name in menores_a_50:\n",
    "        data.append([ column_name, type(comp_data[column_name][0])])\n",
    "print(atributos_ohe)\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "table_df = pd.DataFrame(data, columns=['Column Name', 'Data Type'])\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "print(table_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------\n",
    "# Intento 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATA y drop atribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_comp():\n",
    "    # Load the competition data\n",
    "    comp_data = pd.read_csv(\"../competition_data.csv\")\n",
    "    comp_data = comp_data.drop(\n",
    "        columns=['accepts_mercadopago',\n",
    "                # 'available_quantity',\n",
    "                # 'avg_gmv_item_domain_30days',\n",
    "                # 'avg_gmv_item_sel',\n",
    "                # 'avg_gmv_seller_bday',\n",
    "                # 'avg_qty_orders_item_domain_30days',\n",
    "                # 'avg_qty_orders_item_sel_30days',\n",
    "                # 'avg_si_item_sel_30day',\n",
    "                'benefit',\n",
    "                'boosted',\n",
    "                'category_id',\n",
    "                # 'conversion',\n",
    "                'date',\n",
    "                'deal_print_id',\n",
    "                # 'domain_id',\n",
    "                'etl_version',\n",
    "                # 'free_shipping',\n",
    "                # 'fulfillment',\n",
    "                'full_name',\n",
    "                # 'health',\n",
    "                # 'is_pdp',\n",
    "                # 'product_id',\n",
    "                # 'item_id',\n",
    "                # 'listing_type_id',\n",
    "                # 'logistic_type',\n",
    "                # 'main_picture',\n",
    "                # 'offset',\n",
    "                # 'original_price',\n",
    "                # 'platform',\n",
    "                # 'price',\n",
    "                # 'print_position',\n",
    "                'print_server_timestamp',\n",
    "                # 'qty_items_dom',\n",
    "                # 'qty_items_sel',\n",
    "                # 'site_id',\n",
    "                # 'sold_quantity',\n",
    "                'tags',\n",
    "                'title',\n",
    "                # 'total_asp_item_domain_30days',\n",
    "                # 'total_asp_item_sel_30days',\n",
    "                # 'total_gmv_domain_bday',\n",
    "                # 'total_gmv_item_30days',\n",
    "                # 'total_items_domain',\n",
    "                # 'total_items_seller',\n",
    "                # 'total_orders_domain_30days',\n",
    "                # 'total_orders_item_30days',\n",
    "                # 'total_orders_sel_30days',\n",
    "                # 'total_si_domain_30days',\n",
    "                # 'total_si_item_30days',\n",
    "                # 'total_si_sel_30days',\n",
    "                # 'total_visits_domain',\n",
    "                # 'total_visits_item',\n",
    "                # 'total_visits_seller',\n",
    "                'uid',\n",
    "                'user_id',\n",
    "                'warranty',\n",
    "                # 'ROW_ID'\n",
    "                ]\n",
    "    )\n",
    "    # OHE comp_data columns\n",
    "    comp_data = pd.get_dummies(comp_data, \n",
    "        sparse=True,\n",
    "        columns=[\n",
    "            # \"category_id\", \n",
    "            #\"domain_id\", \n",
    "            \"logistic_type\", \n",
    "            \"platform\", \n",
    "            \"site_id\"\n",
    "        ],\n",
    "        dtype=int\n",
    "    )\n",
    "    #comp_data[\"accepts_mercadopago\"] = comp_data[\"accepts_mercadopago\"].astype(int)\n",
    "    # todos aceptan mercadopago\n",
    "    # comp_data[\"boosted\"] = comp_data[\"boosted\"].astype(int)\n",
    "    comp_data[\"free_shipping\"] = comp_data[\"free_shipping\"].astype(int)\n",
    "    comp_data[\"fulfillment\"] = comp_data[\"fulfillment\"].astype(int)\n",
    "\n",
    "    # comp_data[\"is_pdp\"].fillna(0, inplace=True)\n",
    "    # comp_data[\"is_pdp\"] = comp_data[\"is_pdp\"].astype(int)\n",
    "    #comp_data[\"warranty\"] = comp_data[\"warranty\"].astype(int)\n",
    "\n",
    "    # comp_data[\"listing_type_id\"] to 0 if gold_special, 1 if gold_pro.\n",
    "    comp_data[\"listing_type_id\"] = comp_data[\"listing_type_id\"].apply(lambda x: 0 if x == \"gold_special\" else 1)\n",
    "    \n",
    "    # Label encode category_id and domain_id\n",
    "    # comp_data[\"category_id\"] = comp_data[\"category_id\"].astype(\"category\")#.cat.codes\n",
    "    #comp_data[\"domain_id\"] = comp_data[\"domain_id\"].astype(\"category\")#.cat.codes\n",
    "\n",
    "    # sklearn LabelEncoder for category_id and domain_id\n",
    "    \n",
    "    # comp_data[\"category_id\"] = LabelEncoder().fit_transform(comp_data[\"category_id\"]).astype(int)\n",
    "    comp_data[\"domain_id\"] = LabelEncoder().fit_transform(comp_data[\"domain_id\"]).astype(int)\n",
    "    comp_data[\"item_id\"] = LabelEncoder().fit_transform(comp_data[\"item_id\"]).astype(int)\n",
    "\n",
    "    comp_data[\"price_diff\"] = comp_data[\"price\"] - comp_data[\"original_price\"]\n",
    "    comp_data[\"cheaper_than_original\"] = comp_data[\"price_diff\"].apply(lambda x: 1 if x < 0 else 0)\n",
    "    comp_data[\"price_diff\"] = comp_data[\"price_diff\"].apply(lambda x: abs(x)).astype(int)\n",
    "\n",
    "    #comp_data[\"cheaper_than_avg\"] = comp_data[\"price\"] - comp_data[\"avg_asp_item_domain\"]\n",
    "    #comp_data[\"cheaper_than_avg\"] = comp_data[\"cheaper_than_avg\"].apply(lambda x: 1 if x < 0 else 0).astype(int)\n",
    "\n",
    "    # Drop useless columns\n",
    "    # comp_data = comp_data.drop(\n",
    "    #     columns=[\n",
    "    #     ]\n",
    "    # )\n",
    "\n",
    "    return comp_data\n",
    "comp_data = load_comp()\n",
    "comp_data[\"item_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLIT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and evaluation samples\n",
    "comp_data = load_comp()\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        \"\\n\".join(\n",
    "            [\n",
    "                \"- **{}** ({})\".format(col, dtype) for col, dtype in\n",
    "                zip(comp_data.columns, comp_data.dtypes)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "full_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "\n",
    "# Get number column names for full_data\n",
    "#print(full_data.columns)\n",
    "#num_cols = full_data.select_dtypes(include='number').columns\n",
    "#print(num_cols)\n",
    "# difference between full_data_cols and num_cols\n",
    "# print(set(full_data.columns) - set(num_cols))\n",
    "\n",
    "del comp_data\n",
    "gc.collect()\n",
    "\n",
    "train_data, test_data = train_test_split(full_data, test_size=0.20, train_size=0.80, random_state=42)\n",
    "\n",
    "y_train = train_data[\"conversion\"]\n",
    "X_train = train_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "X_train = X_train.select_dtypes(include='number')\n",
    "\n",
    "y_test = test_data[\"conversion\"]\n",
    "X_test = test_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "X_test = X_test.select_dtypes(include='number')\n",
    "\n",
    "del train_data\n",
    "del test_data\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributes_to_poly = [\n",
    "                        \"available_quantity\", \n",
    "                        \"avg_gmv_item_domain_30days\", \n",
    "                        \"avg_gmv_item_sel\", \n",
    "                        \"avg_gmv_seller_bday\", \n",
    "                        \"avg_qty_orders_item_domain_30days\", \n",
    "                        \"avg_qty_orders_item_sel_30days\", \n",
    "                        \"avg_si_item_sel_30day\",\n",
    "                        \"original_price\",\n",
    "                        \"price\",\n",
    "                    ]\n",
    "\n",
    "imputed_poly_features = make_pipeline(\n",
    "  #SimpleImputer(),\n",
    "  #PolynomialFeatures(),\n",
    "  StandardScaler(),\n",
    "  xgb.XGBClassifier(\n",
    "        missing=np.nan,\n",
    "        n_jobs=-1,\n",
    "        tree_method='gpu_hist',\n",
    "    )\n",
    ")\n",
    "\n",
    "# Custom Layer Starts Here:\n",
    "pl = PolynomialFeatures(2, interaction_only=True, include_bias=False)\n",
    "\n",
    "#Input X contains NaN.\n",
    "\n",
    "imputed_X_train_plus = X_train.copy()\n",
    "imputed_X_test_plus = X_test.copy()\n",
    "print(imputed_X_train_plus.shape[0] == X_train.shape[0])\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "cols_with_missing = (col for col in X_train.columns\n",
    "                        if X_train[col].isnull().any())\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n",
    "    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "poly_X_train = imputed_X_train_plus[atributes_to_poly]\n",
    "poly_X_test = imputed_X_test_plus[atributes_to_poly]\n",
    "print(poly_X_train.shape[0] == X_train.shape[0])\n",
    "\n",
    "poly_X_train = my_imputer.fit_transform(poly_X_train)\n",
    "poly_X_test = my_imputer.transform(poly_X_test)\n",
    "print(poly_X_train.shape[0] == X_train.shape[0])\n",
    "\n",
    "poly_X_train = pl.fit_transform(poly_X_train)\n",
    "poly_X_test = pl.fit_transform(poly_X_test)\n",
    "print(poly_X_train.shape[0] == X_train.shape[0])\n",
    "\n",
    "new_X_train = np.concatenate((X_train.drop(atributes_to_poly, axis=1), poly_X_train), axis=1)\n",
    "new_X_test = np.concatenate((X_test.drop(atributes_to_poly, axis=1), poly_X_test), axis=1)\n",
    "print(new_X_train.shape[0] == X_train.shape[0])\n",
    "# Custom Layer Ends Here\n",
    "\n",
    "\n",
    "imputed_poly_features.fit(new_X_train, y_train)\n",
    "\n",
    "roc_auc_score(y_test, imputed_poly_features.predict_proba(new_X_test)[:, imputed_poly_features.classes_ == 1])\n",
    "# Decision Tree Classifier with K-Fold Cross Validation and Randomized Search CV\n",
    "\n",
    "dtc = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    RandomizedSearchCV(\n",
    "        DecisionTreeClassifier(random_state=2345),\n",
    "        param_distributions={\n",
    "            \"max_depth\": [2, 4, 8, 16, 32, 64, 128, 256, 512, None],\n",
    "            \"min_samples_split\": [2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "            \"min_samples_leaf\": [2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "            \"max_features\": [2, 4, 8, 16, 32, 64, 128, 256, 512, None],\n",
    "        },\n",
    "        n_iter=100,\n",
    "        cv=3,\n",
    "        random_state=2345,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=2345, shuffle=True)\n",
    "roc_aucs = []\n",
    "\n",
    "for train_index, valid_index in kf.split(X_train):\n",
    "    X_train_kf, X_valid_kf = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "    y_train_kf, y_valid_kf = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "\n",
    "    dtc.fit(X_train_kf, y_train_kf)\n",
    "    # Metric: roc_auc_score\n",
    "    roc_aucs.append(roc_auc_score(y_valid_kf, dtc.predict_proba(X_valid_kf)[:, dtc.classes_ == 1]))\n",
    "    print(f\"ROC-AUC split {len(roc_aucs)}: \", roc_aucs[-1])\n",
    "\n",
    "print(\"ROC-AUC mean: \", sum(roc_aucs) / len(roc_aucs))    \n",
    "\n",
    "y_pred = dtc.fit(X_train, y_train)\n",
    "\n",
    "print(\"ROC-AUC: \", roc_auc_score(y_test, dtc.predict_proba(X_test)[:, dtc.classes_ == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"ROW_ID\": eval_data[\"ROW_ID\"], \"conversion\": y_preds})\n",
    "submission_df[\"ROW_ID\"] = submission_df[\"ROW_ID\"].astype(int)\n",
    "submission_df.to_csv(\"exp_oli_1.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------\n",
    "# Intento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data = pd.read_csv(\"../competition_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "comp_data = comp_data.drop(\n",
    "        columns=['accepts_mercadopago',\n",
    "                # 'available_quantity',\n",
    "                # 'avg_gmv_item_domain_30days',\n",
    "                # 'avg_gmv_item_sel',\n",
    "                # 'avg_gmv_seller_bday',\n",
    "                # 'avg_qty_orders_item_domain_30days',\n",
    "                # 'avg_qty_orders_item_sel_30days',\n",
    "                # 'avg_si_item_sel_30day',\n",
    "                'benefit',\n",
    "                'boosted',\n",
    "                'category_id',\n",
    "                # 'conversion',\n",
    "                'date',\n",
    "                'deal_print_id',\n",
    "                'domain_id',\n",
    "                'etl_version',\n",
    "                # 'free_shipping',\n",
    "                # 'fulfillment',\n",
    "                'full_name',\n",
    "                # 'health',\n",
    "                # 'is_pdp',\n",
    "                'product_id',\n",
    "                'item_id',\n",
    "                # 'listing_type_id',\n",
    "                # 'logistic_type',\n",
    "                'main_picture',\n",
    "                # 'offset',\n",
    "                # 'original_price',\n",
    "                'platform',\n",
    "                # 'price',\n",
    "                # 'print_position',\n",
    "                'print_server_timestamp',\n",
    "                # 'qty_items_dom',\n",
    "                # 'qty_items_sel',\n",
    "                'site_id',\n",
    "                # 'sold_quantity',\n",
    "                'tags',\n",
    "                'title',\n",
    "                # 'total_asp_item_domain_30days',\n",
    "                # 'total_asp_item_sel_30days',\n",
    "                # 'total_gmv_domain_bday',\n",
    "                # 'total_gmv_item_30days',\n",
    "                # 'total_items_domain',\n",
    "                # 'total_items_seller',\n",
    "                # 'total_orders_domain_30days',\n",
    "                # 'total_orders_item_30days',\n",
    "                # 'total_orders_sel_30days',\n",
    "                # 'total_si_domain_30days',\n",
    "                # 'total_si_item_30days',\n",
    "                # 'total_si_sel_30days',\n",
    "                # 'total_visits_domain',\n",
    "                # 'total_visits_item',\n",
    "                # 'total_visits_seller',\n",
    "                'uid',\n",
    "                'user_id',\n",
    "                'warranty',\n",
    "                # 'ROW_ID'\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "# eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "\n",
    "# # Get number column names for full_data\n",
    "# #print(full_data.columns)\n",
    "# #num_cols = full_data.select_dtypes(include='number').columns\n",
    "# #print(num_cols)\n",
    "# # difference between full_data_cols and num_cols\n",
    "# # print(set(full_data.columns) - set(num_cols))\n",
    "\n",
    "# del comp_data\n",
    "# gc.collect()\n",
    "\n",
    "# train_data, test_data = train_test_split(full_data, test_size=0.20, train_size=0.80, random_state=42)\n",
    "\n",
    "# y_train = train_data[\"conversion\"]\n",
    "# X_train = train_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "# X_train = X_train.select_dtypes(include='number')\n",
    "\n",
    "# y_test = test_data[\"conversion\"]\n",
    "# X_test = test_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "# X_test = X_test.select_dtypes(include='number')\n",
    "\n",
    "# del train_data\n",
    "# del test_data\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "del comp_data\n",
    "gc.collect()\n",
    "\n",
    "y_train = train_data[\"conversion\"]\n",
    "X_train = train_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "X_train = X_train.select_dtypes(include='number') ## retoque de datos para que haya mas\n",
    "del train_data\n",
    "gc.collect()\n",
    "\n",
    "#hold out set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=3456)\n",
    "y_test = eval_data[\"conversion\"]\n",
    "\n",
    "\n",
    "#OHE comp_data columns\n",
    "# comp_data = pd.get_dummies(comp_data, columns = [])\n",
    "\n",
    "#OHE otra forma \n",
    "# Sobre training\n",
    "X_train_obj = X_train.select_dtypes(include=[\"object\"])\n",
    "X_train_num = X_train.select_dtypes(include=[\"number\"])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder(sparse_output=True, handle_unknown=\"ignore\")\n",
    "X_train_ohe_obj = encoder.fit_transform(X_train_obj)\n",
    "X_train_ohe_v2 = hstack([X_train_ohe_obj, scaler.fit_transform(X_train_num)])\n",
    "\n",
    "# Sobre validation\n",
    "X_val_obj = X_val.select_dtypes(include=[\"object\"])\n",
    "X_val_num = X_val.select_dtypes(include=[\"number\"])\n",
    "\n",
    "X_val_ohe_obj = encoder.transform(X_val_obj)\n",
    "X_val_ohe_v2 = hstack([X_val_ohe_obj, scaler.transform(X_val_num)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#modelo de arbol\n",
    "# Entrenamiento y evaluación del modelo Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=6789, verbose=1, oob_score=True)\n",
    "rf.fit(pd.concat([X_train_ohe_v2, X_val_ohe_v2], axis=0),\n",
    "       pd.concat([y_train, y_val], axis=0))\n",
    "preds_test_rf = rf.predict_proba(X_test)[:, rf.classes_ == True]\n",
    "print(\"ROC test score - Random Forest:\", roc_auc_score(y_test, preds_test_rf)) # 0.9511696343957305\n",
    "\n",
    "# Entrenamiento y evaluación del modelo XGBoost\n",
    "params = {'colsample_bytree': 0.75,\n",
    "               'gamma': 0.5,\n",
    "               'learning_rate': 0.075,\n",
    "               'max_depth': 8,\n",
    "               'min_child_weight': 1,\n",
    "               'n_estimators': 1200,\n",
    "               'reg_lambda': 0.5,\n",
    "               'subsample': 0.75,\n",
    "               }\n",
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic',\n",
    "                            seed = 1234,\n",
    "                            eval_metric = 'auc',\n",
    "                            **params)\n",
    "\n",
    "clf_xgb.fit(X_train_ohe_v2, y_train, verbose = 100, eval_set = [(X_train_ohe_v2, y_train), (X_val_ohe_v2, y_val)])\n",
    "\n",
    "preds_test_xgb = clf_xgb.predict_proba(X_test)[:, clf_xgb.classes_ == True]\n",
    "print(\"AUC test score - XGBoost:\", roc_auc_score(y_test, preds_test_xgb)) # 0.9719437378422864\n",
    "\n",
    "cls = make_pipeline(SimpleImputer(), DecisionTreeClassifier(max_depth=8, random_state=2345))\n",
    "cls.fit(X_train_ohe_v2, y_train)\n",
    "\n",
    "# Predict on the evaluation set\n",
    "# y = eval_data[\"conversion\"]\n",
    "eval_data = eval_data.drop(columns=[\"conversion\"])\n",
    "eval_data = eval_data.select_dtypes(include='number')\n",
    "y_preds = cls.predict_proba(eval_data.drop(columns=[\"ROW_ID\"]))[:, cls.classes_ == 1].squeeze()\n",
    "\n",
    "# kf = KFold(n_splits=5, random_state=2345, shuffle=True)\n",
    "# roc_aucs = []\n",
    "\n",
    "# dtc = make_pipeline(\n",
    "#     SimpleImputer(),\n",
    "#     RandomizedSearchCV(\n",
    "#         DecisionTreeClassifier(random_state=2345),\n",
    "#         param_distributions={\n",
    "#             \"max_depth\": [2, 4, 8, 16, 32, 64, 128, 256, 512, None],\n",
    "#             \"min_samples_split\": [2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "#             \"min_samples_leaf\": [2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "#             \"max_features\": [2, 4, 8, 16, 32, 64, 128, 256, 512, None],\n",
    "#         },\n",
    "#         n_iter=100,\n",
    "#         cv=3,\n",
    "#         random_state=2345,\n",
    "#         n_jobs=-1,\n",
    "#         verbose=0,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# for train_index, valid_index in kf.split(X_train):\n",
    "#     X_train_kf, X_valid_kf = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "#     y_train_kf, y_valid_kf = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "\n",
    "#     dtc.fit(X_train_kf, y_train_kf)\n",
    "#     # Metric: roc_auc_score\n",
    "#     roc_aucs.append(roc_auc_score(y_valid_kf, dtc.predict_proba(X_valid_kf)[:, dtc.classes_ == 1]))\n",
    "#     print(f\"ROC-AUC split {len(roc_aucs)}: \", roc_aucs[-1])\n",
    "\n",
    "# print(\"ROC-AUC mean: \", sum(roc_aucs) / len(roc_aucs))    \n",
    "\n",
    "# dtc.fit(X_train, y_train)\n",
    "\n",
    "# print(\"ROC-AUC: \", roc_auc_score(y_test, dtc.predict_proba(X_test)[:, dtc.classes_ == 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the submission file\n",
    "submission_df = pd.DataFrame({\"ROW_ID\": eval_data[\"ROW_ID\"], \"conversion\": y_preds})\n",
    "submission_df[\"ROW_ID\"] = submission_df[\"ROW_ID\"].astype(int)\n",
    "submission_df.to_csv(\"basic_model_oli.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_val,y_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------\n",
    "# INTENTO 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data = pd.read_csv(\"./competition_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "del comp_data\n",
    "gc.collect()\n",
    "\n",
    "y_train = train_data[\"conversion\"]\n",
    "X_train = train_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "X_train = X_train.select_dtypes(include='number') ## retoque de datos para que haya mas\n",
    "del train_data\n",
    "gc.collect()\n",
    "\n",
    "#hold out set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=3456)\n",
    "y_test = eval_data[\"conversion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['benefit', 'health']\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'X_train' is your DataFrame\n",
    "atributos_ohe = []\n",
    "\n",
    "for i, column_name in enumerate(X_train.columns):\n",
    "    if isinstance(X_train[column_name][0], (float, int)) and column_name in menores_a_50:\n",
    "        # atributos_ohe.append([ column_name, type(X_train[column_name][0])])\n",
    "        atributos_ohe.append(column_name)\n",
    "print(atributos_ohe)\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "# table_df = pd.DataFrame(atributos_ohe, columns=['Column Name', 'Data Type'])\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "# print(table_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False health_0.36 health_0.4\n",
      "False health_0.4 health_0.42\n",
      "False health_0.42 health_0.44\n",
      "False health_0.44 health_0.45\n",
      "False health_0.45 health_0.5\n",
      "False health_0.5 health_0.53\n"
     ]
    }
   ],
   "source": [
    "X_train_ohe = pd.get_dummies(X_train,\n",
    "                        columns = atributos_ohe,\n",
    "                        sparse = True,    # Devolver una matriz rala.\n",
    "                        dummy_na = False, # No agregar columna para NaNs.\n",
    "                        dtype = int       # XGBoost no trabaja con 'object'; necesitamos que sean numéricos.\n",
    "                       )\n",
    "# X_train_ohe\n",
    "\n",
    "X_val_ohe = pd.get_dummies(X_val,                        \n",
    "                        columns = atributos_ohe,\n",
    "                        sparse = True,    # Devolver una matriz rala.\n",
    "                        dummy_na = False, # No agregar columna para NaNs.\n",
    "                        dtype = int       # XGBoost no trabaja con 'object'; necesitamos que sean numéricos.\n",
    "                       )\n",
    "\n",
    "for columns_train, columns_val in zip(X_train_ohe.columns, X_val_ohe.columns):\n",
    "    if columns_train != columns_val:\n",
    "        print(False, columns_train, columns_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>available_quantity</th>\n",
       "      <th>avg_gmv_item_domain_30days</th>\n",
       "      <th>avg_gmv_item_sel</th>\n",
       "      <th>avg_gmv_seller_bday</th>\n",
       "      <th>avg_qty_orders_item_domain_30days</th>\n",
       "      <th>avg_qty_orders_item_sel_30days</th>\n",
       "      <th>avg_si_item_sel_30day</th>\n",
       "      <th>product_id</th>\n",
       "      <th>offset</th>\n",
       "      <th>original_price</th>\n",
       "      <th>...</th>\n",
       "      <th>health_0.81</th>\n",
       "      <th>health_0.83</th>\n",
       "      <th>health_0.84</th>\n",
       "      <th>health_0.85</th>\n",
       "      <th>health_0.87</th>\n",
       "      <th>health_0.88</th>\n",
       "      <th>health_0.9</th>\n",
       "      <th>health_0.91</th>\n",
       "      <th>health_0.92</th>\n",
       "      <th>health_1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13223</th>\n",
       "      <td>20</td>\n",
       "      <td>203.751822</td>\n",
       "      <td>273.334509</td>\n",
       "      <td>1576.229000</td>\n",
       "      <td>3.637863</td>\n",
       "      <td>4.670520</td>\n",
       "      <td>5.063584</td>\n",
       "      <td>7571378.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3199</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121161</th>\n",
       "      <td>90398</td>\n",
       "      <td>573.574281</td>\n",
       "      <td>726.854545</td>\n",
       "      <td>266.513333</td>\n",
       "      <td>9.578684</td>\n",
       "      <td>32.454545</td>\n",
       "      <td>85.454545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>384</td>\n",
       "      <td>650</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9020</th>\n",
       "      <td>18</td>\n",
       "      <td>232.644091</td>\n",
       "      <td>180.850464</td>\n",
       "      <td>2206.375667</td>\n",
       "      <td>6.140316</td>\n",
       "      <td>8.374317</td>\n",
       "      <td>15.191257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144</td>\n",
       "      <td>1999</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166398</th>\n",
       "      <td>234</td>\n",
       "      <td>59.801321</td>\n",
       "      <td>169.766875</td>\n",
       "      <td>181.084667</td>\n",
       "      <td>2.622240</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>22.875000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>192</td>\n",
       "      <td>840</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95268</th>\n",
       "      <td>7</td>\n",
       "      <td>60.570196</td>\n",
       "      <td>195.463231</td>\n",
       "      <td>1694.014667</td>\n",
       "      <td>2.737064</td>\n",
       "      <td>4.188462</td>\n",
       "      <td>5.630769</td>\n",
       "      <td>NaN</td>\n",
       "      <td>192</td>\n",
       "      <td>680</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        available_quantity  avg_gmv_item_domain_30days  avg_gmv_item_sel   \n",
       "13223                   20                  203.751822        273.334509  \\\n",
       "121161               90398                  573.574281        726.854545   \n",
       "9020                    18                  232.644091        180.850464   \n",
       "166398                 234                   59.801321        169.766875   \n",
       "95268                    7                   60.570196        195.463231   \n",
       "\n",
       "        avg_gmv_seller_bday  avg_qty_orders_item_domain_30days   \n",
       "13223           1576.229000                           3.637863  \\\n",
       "121161           266.513333                           9.578684   \n",
       "9020            2206.375667                           6.140316   \n",
       "166398           181.084667                           2.622240   \n",
       "95268           1694.014667                           2.737064   \n",
       "\n",
       "        avg_qty_orders_item_sel_30days  avg_si_item_sel_30day  product_id   \n",
       "13223                         4.670520               5.063584   7571378.0  \\\n",
       "121161                       32.454545              85.454545         NaN   \n",
       "9020                          8.374317              15.191257         NaN   \n",
       "166398                       13.500000              22.875000         NaN   \n",
       "95268                         4.188462               5.630769         NaN   \n",
       "\n",
       "        offset  original_price  ...  health_0.81  health_0.83  health_0.84   \n",
       "13223        0            3199  ...            0            0            0  \\\n",
       "121161     384             650  ...            0            0            0   \n",
       "9020       144            1999  ...            0            0            0   \n",
       "166398     192             840  ...            0            0            0   \n",
       "95268      192             680  ...            0            0            0   \n",
       "\n",
       "        health_0.85  health_0.87  health_0.88  health_0.9  health_0.91   \n",
       "13223             0            0            1           0            0  \\\n",
       "121161            0            0            0           0            0   \n",
       "9020              0            0            0           0            0   \n",
       "166398            0            0            0           0            0   \n",
       "95268             0            0            0           0            0   \n",
       "\n",
       "        health_0.92  health_1.0  \n",
       "13223             0           0  \n",
       "121161            0           1  \n",
       "9020              0           0  \n",
       "166398            0           0  \n",
       "95268             0           0  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['avg_gmv_item_domain_30days', 'avg_gmv_item_sel', 'avg_gmv_seller_bday', 'avg_qty_orders_item_domain_30days', 'avg_qty_orders_item_sel_30days', 'avg_si_item_sel_30day', 'product_id', 'qty_items_dom', 'qty_items_sel', 'total_asp_item_domain_30days', 'total_asp_item_sel_30days', 'total_gmv_domain_bday', 'total_gmv_item_30days', 'total_orders_domain_30days', 'total_orders_item_30days', 'total_orders_sel_30days', 'total_si_domain_30days', 'total_si_item_30days', 'total_si_sel_30days', 'user_id', 'benefit_0.0', 'health_0.36', 'health_0.4', 'health_0.42', 'health_0.44', 'health_0.45', 'health_0.5', 'health_0.54', 'health_0.55', 'health_0.57', 'health_0.58', 'health_0.6', 'health_0.61', 'health_0.62', 'health_0.63', 'health_0.66', 'health_0.69', 'health_0.7', 'health_0.71', 'health_0.72', 'health_0.75', 'health_0.76', 'health_0.77', 'health_0.8', 'health_0.81', 'health_0.83', 'health_0.84', 'health_0.85', 'health_0.87', 'health_0.88', 'health_0.9', 'health_0.91', 'health_0.92', 'health_1.0']\n"
     ]
    }
   ],
   "source": [
    "testing = []\n",
    "for i, column_name in enumerate(X_train_ohe.columns):\n",
    "    if isinstance(X_train_ohe[column_name][0], (float, int)):\n",
    "        # testing.append([ column_name, type(comp_data_ohe[column_name][0])])\n",
    "        testing.append(column_name)\n",
    "print(testing)\n",
    "\n",
    "# # Create a DataFrame from the data\n",
    "# table_df = pd.DataFrame(testing, columns=['Column Name', 'Data Type'])\n",
    "\n",
    "# # Display the DataFrame as a table\n",
    "# print(table_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_erase = []\n",
    "for columns in X_train_ohe:\n",
    "    if columns not in testing:\n",
    "        columns_to_erase.append(columns)\n",
    "\n",
    "X_train_i3 = X_train_ohe.drop(columns_to_erase, axis=1)\n",
    "X_val_i3 = X_val_ohe.drop(columns_to_erase, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['avg_gmv_item_domain_30days', 'avg_gmv_item_sel', 'avg_gmv_seller_bday', 'avg_qty_orders_item_domain_30days', 'avg_qty_orders_item_sel_30days', 'avg_si_item_sel_30day', 'product_id', 'qty_items_dom', 'qty_items_sel', 'total_asp_item_domain_30days', 'total_asp_item_sel_30days', 'total_gmv_domain_bday', 'total_gmv_item_30days', 'total_orders_domain_30days', 'total_orders_item_30days', 'total_orders_sel_30days', 'total_si_domain_30days', 'total_si_item_30days', 'total_si_sel_30days', 'user_id', 'benefit_0.0', 'health_0.36', 'health_0.4', 'health_0.42', 'health_0.44', 'health_0.45', 'health_0.5', 'health_0.54', 'health_0.55', 'health_0.57', 'health_0.58', 'health_0.6', 'health_0.61', 'health_0.62', 'health_0.63', 'health_0.66', 'health_0.69', 'health_0.7', 'health_0.71', 'health_0.72', 'health_0.75', 'health_0.76', 'health_0.77', 'health_0.8', 'health_0.81', 'health_0.83', 'health_0.84', 'health_0.85', 'health_0.87', 'health_0.88', 'health_0.9', 'health_0.91', 'health_0.92', 'health_1.0', 'health_0.53'] ['avg_gmv_item_domain_30days', 'avg_gmv_item_sel', 'avg_gmv_seller_bday', 'avg_qty_orders_item_domain_30days', 'avg_qty_orders_item_sel_30days', 'avg_si_item_sel_30day', 'product_id', 'qty_items_dom', 'qty_items_sel', 'total_asp_item_domain_30days', 'total_asp_item_sel_30days', 'total_gmv_domain_bday', 'total_gmv_item_30days', 'total_orders_domain_30days', 'total_orders_item_30days', 'total_orders_sel_30days', 'total_si_domain_30days', 'total_si_item_30days', 'total_si_sel_30days', 'user_id', 'benefit_0.0', 'health_0.4', 'health_0.42', 'health_0.44', 'health_0.45', 'health_0.5', 'health_0.53', 'health_0.54', 'health_0.55', 'health_0.57', 'health_0.58', 'health_0.6', 'health_0.61', 'health_0.62', 'health_0.63', 'health_0.66', 'health_0.69', 'health_0.7', 'health_0.71', 'health_0.72', 'health_0.75', 'health_0.76', 'health_0.77', 'health_0.8', 'health_0.81', 'health_0.83', 'health_0.84', 'health_0.85', 'health_0.87', 'health_0.88', 'health_0.9', 'health_0.91', 'health_0.92', 'health_1.0', 'health_0.36']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/valen/Desktop/Ditella/Año 3 sem 2/TD6/tp2_td6/experimentacion_oli.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valen/Desktop/Ditella/A%C3%B1o%203%20sem%202/TD6/tp2_td6/experimentacion_oli.ipynb#X46sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(random_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valen/Desktop/Ditella/A%C3%B1o%203%20sem%202/TD6/tp2_td6/experimentacion_oli.ipynb#X46sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m clf_xgb \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBClassifier(objective\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary:logistic\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valen/Desktop/Ditella/A%C3%B1o%203%20sem%202/TD6/tp2_td6/experimentacion_oli.ipynb#X46sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m                             seed\u001b[39m=\u001b[39mrandom_state,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valen/Desktop/Ditella/A%C3%B1o%203%20sem%202/TD6/tp2_td6/experimentacion_oli.ipynb#X46sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m                             eval_metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauc\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/valen/Desktop/Ditella/A%C3%B1o%203%20sem%202/TD6/tp2_td6/experimentacion_oli.ipynb#X46sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m clf_xgb\u001b[39m.\u001b[39;49mfit(X_train_i3, y_train, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, eval_set\u001b[39m=\u001b[39;49m[(X_val_i3, y_val)])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1462\u001b[0m (\n\u001b[1;32m   1463\u001b[0m     model,\n\u001b[1;32m   1464\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1470\u001b[0m )\n\u001b[1;32m   1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1473\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[1;32m   1488\u001b[0m )\n\u001b[0;32m-> 1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1491\u001b[0m     params,\n\u001b[1;32m   1492\u001b[0m     train_dmatrix,\n\u001b[1;32m   1493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1494\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1495\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1496\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1497\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1498\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1499\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1500\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1502\u001b[0m )\n\u001b[1;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/training.py:160\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    157\u001b[0m metric_fn \u001b[39m=\u001b[39m _configure_custom_metric(feval, custom_metric)\n\u001b[1;32m    158\u001b[0m evals \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(evals) \u001b[39mif\u001b[39;00m evals \u001b[39melse\u001b[39;00m []\n\u001b[0;32m--> 160\u001b[0m bst \u001b[39m=\u001b[39m Booster(params, [dtrain] \u001b[39m+\u001b[39;49m [d[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m evals], model_file\u001b[39m=\u001b[39;49mxgb_model)\n\u001b[1;32m    161\u001b[0m start_iteration \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    163\u001b[0m _assert_new_callback(callbacks)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:1556\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, cache, model_file)\u001b[0m\n\u001b[1;32m   1552\u001b[0m _check_call(_LIB\u001b[39m.\u001b[39mXGBoosterCreate(dmats, c_bst_ulong(\u001b[39mlen\u001b[39m(cache)),\n\u001b[1;32m   1553\u001b[0m                                  ctypes\u001b[39m.\u001b[39mbyref(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle)))\n\u001b[1;32m   1554\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m cache:\n\u001b[1;32m   1555\u001b[0m     \u001b[39m# Validate feature only after the feature names are saved into booster.\u001b[39;00m\n\u001b[0;32m-> 1556\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_dmatrix_features(d)\n\u001b[1;32m   1558\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model_file, Booster):\n\u001b[1;32m   1559\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:2747\u001b[0m, in \u001b[0;36mBooster._validate_dmatrix_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   2744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2745\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types \u001b[39m=\u001b[39m ft\n\u001b[0;32m-> 2747\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_features(fn)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:2782\u001b[0m, in \u001b[0;36mBooster._validate_features\u001b[0;34m(self, feature_names)\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[39mif\u001b[39;00m my_missing:\n\u001b[1;32m   2777\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m   2778\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mtraining data did not have the following fields: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2779\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m my_missing)\n\u001b[1;32m   2780\u001b[0m     )\n\u001b[0;32m-> 2782\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names, feature_names))\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['avg_gmv_item_domain_30days', 'avg_gmv_item_sel', 'avg_gmv_seller_bday', 'avg_qty_orders_item_domain_30days', 'avg_qty_orders_item_sel_30days', 'avg_si_item_sel_30day', 'product_id', 'qty_items_dom', 'qty_items_sel', 'total_asp_item_domain_30days', 'total_asp_item_sel_30days', 'total_gmv_domain_bday', 'total_gmv_item_30days', 'total_orders_domain_30days', 'total_orders_item_30days', 'total_orders_sel_30days', 'total_si_domain_30days', 'total_si_item_30days', 'total_si_sel_30days', 'user_id', 'benefit_0.0', 'health_0.36', 'health_0.4', 'health_0.42', 'health_0.44', 'health_0.45', 'health_0.5', 'health_0.54', 'health_0.55', 'health_0.57', 'health_0.58', 'health_0.6', 'health_0.61', 'health_0.62', 'health_0.63', 'health_0.66', 'health_0.69', 'health_0.7', 'health_0.71', 'health_0.72', 'health_0.75', 'health_0.76', 'health_0.77', 'health_0.8', 'health_0.81', 'health_0.83', 'health_0.84', 'health_0.85', 'health_0.87', 'health_0.88', 'health_0.9', 'health_0.91', 'health_0.92', 'health_1.0', 'health_0.53'] ['avg_gmv_item_domain_30days', 'avg_gmv_item_sel', 'avg_gmv_seller_bday', 'avg_qty_orders_item_domain_30days', 'avg_qty_orders_item_sel_30days', 'avg_si_item_sel_30day', 'product_id', 'qty_items_dom', 'qty_items_sel', 'total_asp_item_domain_30days', 'total_asp_item_sel_30days', 'total_gmv_domain_bday', 'total_gmv_item_30days', 'total_orders_domain_30days', 'total_orders_item_30days', 'total_orders_sel_30days', 'total_si_domain_30days', 'total_si_item_30days', 'total_si_sel_30days', 'user_id', 'benefit_0.0', 'health_0.4', 'health_0.42', 'health_0.44', 'health_0.45', 'health_0.5', 'health_0.53', 'health_0.54', 'health_0.55', 'health_0.57', 'health_0.58', 'health_0.6', 'health_0.61', 'health_0.62', 'health_0.63', 'health_0.66', 'health_0.69', 'health_0.7', 'health_0.71', 'health_0.72', 'health_0.75', 'health_0.76', 'health_0.77', 'health_0.8', 'health_0.81', 'health_0.83', 'health_0.84', 'health_0.85', 'health_0.87', 'health_0.88', 'health_0.9', 'health_0.91', 'health_0.92', 'health_1.0', 'health_0.36']"
     ]
    }
   ],
   "source": [
    "from scipy.stats import uniform\n",
    "# Verifica si la columna 'health_0.53' está en X_train_i3\n",
    "if 'health_0.53' not in X_train_i3.columns:\n",
    "    # Si no está presente, agrégala y llénala con ceros\n",
    "    X_train_i3['health_0.53'] = 0\n",
    "\n",
    "\n",
    "params = {'max_depth': list(range(1, 40)),\n",
    "          'learning_rate': uniform(scale=0.2),\n",
    "          'gamma': uniform(scale=2),\n",
    "          'reg_lambda': uniform(scale=5),\n",
    "          'subsample': uniform(0.5, 0.5),\n",
    "          'min_child_weight': uniform(scale=5),\n",
    "          'colsample_bytree': uniform(0.75, 0.25),\n",
    "          'n_estimators': list(range(1, 1000))\n",
    "         }\n",
    "\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                            seed=random_state,\n",
    "                            eval_metric='auc')\n",
    "\n",
    "clf_xgb.fit(X_train_i3, y_train, verbose=True, eval_set=[(X_val_i3, y_val)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "best_score = 0\n",
    "best_estimator = None\n",
    "iterations = 100\n",
    "for g in ParameterSampler(params, n_iter = iterations, random_state = random_state):\n",
    "    clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic', seed = random_state, eval_metric = 'auc', **g)\n",
    "    clf_xgb.fit(X_train, Y_train, eval_set = [(X_val, Y_val)], verbose = False)\n",
    "\n",
    "    y_pred = clf_xgb.predict_proba(X_val)[:, 1] # Obtenemos la probabilidad de una de las clases (cualquiera).\n",
    "    auc_roc = sklearn.metrics.roc_auc_score(Y_val, y_pred)\n",
    "    # Guardamos si es mejor.\n",
    "    if auc_roc > best_score:\n",
    "        print(f'Mejor valor de ROC-AUC encontrado: {auc_roc}')\n",
    "        best_score = auc_roc\n",
    "        best_grid = g\n",
    "        best_estimator = clf_xgb\n",
    "\n",
    "end = time.time()\n",
    "print('ROC-AUC: %0.5f' % best_score)\n",
    "print('Grilla:', best_grid)\n",
    "print(f'Tiempo transcurrido: {str(end - start)} segundos')\n",
    "print(f'Tiempo de entrenamiento por iteración: {str(round((end - start) / iterations, 2))} segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_estimator.predict_proba(X_test)[:, 1]\n",
    "auc_roc = sklearn.metrics.roc_auc_score(Y_test, y_pred)\n",
    "print('AUC-ROC test: %0.5f' % auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------\n",
    "# CODIGO AYUDA \n",
    "### KFOLD CLASE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross validation\n",
    "kcv = KFold(n_splits=10, shuffle=True)\n",
    "rmse_kcv1 = []\n",
    "for train_ix, test_ix in tqdm(kcv.split(X_train), total=10):\n",
    "    X_train_red, X_val = X_train.iloc[train_ix,:], X_train.iloc[test_ix,:]\n",
    "    y_train_red, y_val = y_train[train_ix], y_train[test_ix]\n",
    "    tree = DecisionTreeRegressor(max_depth=10, random_state=TREE_SEED)\n",
    "    tree.fit(X_train_red, y_train_red)\n",
    "    preds_val = tree.predict(X_val)\n",
    "    rmse_kcv1.append(math.sqrt(mean_squared_error(y_val, preds_val)))\n",
    "\n",
    "print(f\"RMSE estimado mediante k-fold CV (Versión 1): {mean(rmse_kcv1):.2f}\")\n",
    "\n",
    "# Validación cruzada k-fold utilizando cross_val_score\n",
    "kcv = KFold(n_splits=10, shuffle=True)\n",
    "tree = DecisionTreeRegressor(max_depth=10, random_state=TREE_SEED)\n",
    "rmse_kcv2 = cross_val_score(tree, X_train, y_train, scoring='neg_root_mean_squared_error', cv=kcv, n_jobs=-1)\n",
    "\n",
    "print(f\"RMSE estimado mediante k-fold CV (Versión 2): {-1 * mean(rmse_kcv2):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "comp_data = comp_data.drop(\n",
    "        columns=['accepts_mercadopago',\n",
    "                # 'available_quantity',\n",
    "                # 'avg_gmv_item_domain_30days',\n",
    "                # 'avg_gmv_item_sel',\n",
    "                # 'avg_gmv_seller_bday',\n",
    "                # 'avg_qty_orders_item_domain_30days',\n",
    "                # 'avg_qty_orders_item_sel_30days',\n",
    "                # 'avg_si_item_sel_30day',\n",
    "                'benefit',\n",
    "                'boosted',\n",
    "                'category_id',\n",
    "                # 'conversion',\n",
    "                'date',\n",
    "                'deal_print_id',\n",
    "                'domain_id',\n",
    "                'etl_version',\n",
    "                # 'free_shipping',\n",
    "                # 'fulfillment',\n",
    "                'full_name',\n",
    "                # 'health',\n",
    "                # 'is_pdp',\n",
    "                'product_id',\n",
    "                'item_id',\n",
    "                # 'listing_type_id',\n",
    "                # 'logistic_type',\n",
    "                'main_picture',\n",
    "                # 'offset',\n",
    "                # 'original_price',\n",
    "                'platform',\n",
    "                # 'price',\n",
    "                # 'print_position',\n",
    "                'print_server_timestamp',\n",
    "                # 'qty_items_dom',\n",
    "                # 'qty_items_sel',\n",
    "                'site_id',\n",
    "                # 'sold_quantity',\n",
    "                'tags',\n",
    "                'title',\n",
    "                # 'total_asp_item_domain_30days',\n",
    "                # 'total_asp_item_sel_30days',\n",
    "                # 'total_gmv_domain_bday',\n",
    "                # 'total_gmv_item_30days',\n",
    "                # 'total_items_domain',\n",
    "                # 'total_items_seller',\n",
    "                # 'total_orders_domain_30days',\n",
    "                # 'total_orders_item_30days',\n",
    "                # 'total_orders_sel_30days',\n",
    "                # 'total_si_domain_30days',\n",
    "                # 'total_si_item_30days',\n",
    "                # 'total_si_sel_30days',\n",
    "                # 'total_visits_domain',\n",
    "                # 'total_visits_item',\n",
    "                # 'total_visits_seller',\n",
    "                'uid',\n",
    "                'user_id',\n",
    "                'warranty',\n",
    "                # 'ROW_ID'\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hago XGboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "no se si usar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, holdout y testing sets\n",
    "size_val = math.ceil(0.1 * X_train.shape[0])\n",
    "size_test = math.ceil(0.1 * X_train.shape[0])\n",
    "\n",
    "X_train_red, X_test, y_train_red, y_test = train_test_split(X_train, y_train, test_size=size_test)\n",
    "X_train_red, X_val, y_train_red, y_val = train_test_split(X_train_red, y_train_red, test_size=size_val)\n",
    "\n",
    "# Prueba con distintos niveles de profundidad (model selection)\n",
    "exp_results = []\n",
    "for md in tqdm(range(1, 51)):\n",
    "    tree = DecisionTreeRegressor(max_depth=md, random_state=TREE_SEED)\n",
    "    tree.fit(X_train_red, y_train_red)\n",
    "    preds_val = tree.predict(X_val)\n",
    "    exp_results.append({\"max_depth\": md,\n",
    "                        \"rmse_val\": math.sqrt(mean_squared_error(y_val, preds_val))})\n",
    "\n",
    "exp_results = pd.DataFrame(exp_results)\n",
    "plot_exp(exp_results)\n",
    "\n",
    "# Se entrena el árbol con la mejor profundidad encontrada sobre train set + validation set\n",
    "best_md = exp_results[exp_results[\"rmse_val\"].min() == exp_results[\"rmse_val\"]]\n",
    "best_md = best_md.sort_values(\"max_depth\").iloc[0,:]\n",
    "print(f\"Performance del mejor modelos: {best_md['rmse_val']:.2f}\")\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=int(best_md[\"max_depth\"]), random_state=TREE_SEED)\n",
    "tree.fit(pd.concat([X_train_red, X_val], axis=0),\n",
    "         pd.concat([y_train_red, y_val], axis=0))\n",
    "preds_test = tree.predict(X_test)\n",
    "\n",
    "print(f\"RMSE estimado en test: {math.sqrt(mean_squared_error(y_test, preds_test)):.2f}\")\n",
    "\n",
    "# Evaluación final en el conjunto de evaluación (model assestment)\n",
    "tree = DecisionTreeRegressor(max_depth=int(best_md[\"max_depth\"]), random_state=TREE_SEED)\n",
    "tree.fit(X_train, y_train)\n",
    "preds_eval = tree.predict(X_eval)\n",
    "_, _, _, y_eval = load_data()\n",
    "\n",
    "print(f\"Performance de evaluación: {math.sqrt(mean_squared_error(y_eval, preds_eval)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, precision_recall_fscore_support, precision_recall_curve, average_precision_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# One-hot-encoding\n",
    "# Opción 1: utilizar get_dummies de pandas\n",
    "X_train_ohe_v1 = pd.get_dummies(X_train).head()  # Y ahora cómo lo aplico en validación? (debería unir antes los datos)\n",
    "\n",
    "# Opción 2: utilizar onehotencoder de sklearn (más conveniente para producción)\n",
    "\n",
    "# Sobre training\n",
    "X_train_obj = X_train.select_dtypes(include=[\"object\"])\n",
    "X_train_num = X_train.select_dtypes(include=[\"int\", \"float\"])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder(sparse_output=True)\n",
    "X_train_ohe_obj = encoder.fit_transform(X_train_obj)\n",
    "X_train_ohe_v2 = hstack([X_train_ohe_obj, scaler.fit_transform(X_train_num)])\n",
    "\n",
    "# Sobre validation\n",
    "X_val_obj = X_val.select_dtypes(include=[\"object\"])\n",
    "X_val_num = X_val.select_dtypes(include=[\"int\", \"float\"])\n",
    "\n",
    "X_val_ohe_obj = encoder.transform(X_val_obj)\n",
    "X_val_ohe_v2 = hstack([X_val_ohe_obj, scaler.transform(X_val_num)])\n",
    "\n",
    "# Entrenamos un modelo de regresión logística sobre los datos con OHE sin escalado\n",
    "lr_exp = []\n",
    "for C in tqdm([0.001, 0.0025, 0.01, 0.05, 0.075, 0.1, 0.25]):\n",
    "    lr = LogisticRegression(C=C, max_iter=2000)\n",
    "    lr.fit(X_train_ohe_v2, y_train)\n",
    "    preds = lr.predict_proba(X_val_ohe_v2)\n",
    "    preds = preds[:, lr.classes_ == \"yes\"]\n",
    "    auc = roc_auc_score(y_val == \"yes\", preds)\n",
    "    lr_exp.append({\"C\": C, \"auc_val\": auc})\n",
    "\n",
    "lr_exp = pd.DataFrame(lr_exp)\n",
    "print(lr_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

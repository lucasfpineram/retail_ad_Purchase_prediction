{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, KFold, cross_val_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures, StandardScaler, OneHotEncoder\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(['accepts_mercadopago',\n",
    "    # 'available_quantity',\n",
    "    # 'avg_gmv_item_domain_30days',\n",
    "    # 'avg_gmv_item_sel',\n",
    "    # 'avg_gmv_seller_bday',\n",
    "    # 'avg_qty_orders_item_domain_30days',\n",
    "    # 'avg_qty_orders_item_sel_30days',\n",
    "    # 'avg_si_item_sel_30day',\n",
    "    'benefit',\n",
    "    'boosted',\n",
    "    # 'category_id',\n",
    "    # 'conversion',\n",
    "    'date',\n",
    "    'deal_print_id',\n",
    "    # 'domain_id',\n",
    "    'etl_version',\n",
    "    # 'free_shipping',\n",
    "    # 'fulfillment',\n",
    "    'full_name',\n",
    "    # 'health',\n",
    "    'is_pdp',\n",
    "    # 'product_id',\n",
    "    # 'item_id',\n",
    "    # 'listing_type_id',\n",
    "    # 'logistic_type',\n",
    "    # 'main_picture',\n",
    "    # 'offset',\n",
    "    # 'original_price',\n",
    "    # 'platform',\n",
    "    # 'price',\n",
    "    # 'print_position',\n",
    "    'print_server_timestamp',\n",
    "    # 'qty_items_dom',\n",
    "    # 'qty_items_sel',\n",
    "    'site_id',\n",
    "    # 'sold_quantity',\n",
    "    'tags',\n",
    "    'title',\n",
    "    # 'total_asp_item_domain_30days',\n",
    "    # 'total_asp_item_sel_30days',\n",
    "    # 'total_gmv_domain_bday',\n",
    "    # 'total_gmv_item_30days',\n",
    "    # 'total_items_domain',\n",
    "    # 'total_items_seller',\n",
    "    # 'total_orders_domain_30days',\n",
    "    # 'total_orders_item_30days',\n",
    "    # 'total_orders_sel_30days',\n",
    "    # 'total_si_domain_30days',\n",
    "    # 'total_si_item_30days',\n",
    "    # 'total_si_sel_30days',\n",
    "    # 'total_visits_domain',\n",
    "    # 'total_visits_item',\n",
    "    # 'total_visits_seller',\n",
    "    'uid',\n",
    "    'user_id',\n",
    "    'warranty',\n",
    "    'ROW_ID'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio de los atributos\n",
    "nÂ° - atributo = # cantidad de uniques\n",
    "- 0 accepts_mercadopago 1\n",
    "- 8 benefit 3\n",
    "- 9 boosted 1\n",
    "- 12 date 31\n",
    "- 13 deal_print_id 196744\n",
    "- 15 etl_version 1\n",
    "- 20 is_pdp 3\n",
    "- 31 print_server_timestamp 196780\n",
    "- 34 site_id 1\n",
    "- 36 tags 1227\n",
    "- 37 title 26745\n",
    "- 53 uid 159996\n",
    "- 54 user_id 139111\n",
    "- 55 warranty 1289\n",
    "- 56 ROW_ID 19212 \n",
    "199972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 accepts_mercadopago 1\n",
      "1 available_quantity 6937\n",
      "2 avg_gmv_item_domain_30days 17728\n",
      "3 avg_gmv_item_sel 18653\n",
      "4 avg_gmv_seller_bday 18635\n",
      "5 avg_qty_orders_item_domain_30days 16761\n",
      "6 avg_qty_orders_item_sel_30days 15130\n",
      "7 avg_si_item_sel_30day 15528\n",
      "8 benefit 3\n",
      "9 boosted 1\n",
      "10 category_id 2284\n",
      "11 conversion 3\n",
      "12 date 31\n",
      "13 deal_print_id 196744\n",
      "14 domain_id 1742\n",
      "15 etl_version 1\n",
      "16 free_shipping 2\n",
      "17 fulfillment 2\n",
      "18 full_name 2280\n",
      "19 health 35\n",
      "20 is_pdp 3\n",
      "21 product_id 4441\n",
      "22 item_id 27695\n",
      "23 listing_type_id 2\n",
      "24 logistic_type 7\n",
      "25 main_picture 26962\n",
      "26 offset 209\n",
      "27 original_price 6469\n",
      "28 platform 4\n",
      "29 price 6029\n",
      "30 print_position 10902\n",
      "31 print_server_timestamp 196780\n",
      "32 qty_items_dom 4194\n",
      "33 qty_items_sel 1463\n",
      "34 site_id 1\n",
      "35 sold_quantity 5718\n",
      "36 tags 1227\n",
      "37 title 26745\n",
      "38 total_asp_item_domain_30days 17730\n",
      "39 total_asp_item_sel_30days 18652\n",
      "40 total_gmv_domain_bday 17718\n",
      "41 total_gmv_item_30days 45133\n",
      "42 total_items_domain 10309\n",
      "43 total_items_seller 3240\n",
      "44 total_orders_domain_30days 7883\n",
      "45 total_orders_item_30days 636\n",
      "46 total_orders_sel_30days 4414\n",
      "47 total_si_domain_30days 9158\n",
      "48 total_si_item_30days 851\n",
      "49 total_si_sel_30days 5302\n",
      "50 total_visits_domain 17646\n",
      "51 total_visits_item 9618\n",
      "52 total_visits_seller 17762\n",
      "53 uid 159996\n",
      "54 user_id 139111\n",
      "55 warranty 1289\n",
      "56 ROW_ID 19212\n",
      "199972\n"
     ]
    }
   ],
   "source": [
    "comp_data = pd.read_csv(\"../competition_data.csv\")\n",
    "for i in range(len(comp_data.columns)):\n",
    "    print(i,comp_data.columns[i], len(comp_data[comp_data.columns[i]].unique()))\n",
    "print(len(comp_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------\n",
    "# Intento 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATA y drop atribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_id\n",
       "14540    717\n",
       "11932    701\n",
       "27372    622\n",
       "14900    535\n",
       "12183    517\n",
       "        ... \n",
       "23949      1\n",
       "3863       1\n",
       "14492      1\n",
       "18426      1\n",
       "11527      1\n",
       "Name: count, Length: 27695, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_comp():\n",
    "    # Load the competition data\n",
    "    comp_data = pd.read_csv(\"../competition_data.csv\")\n",
    "    comp_data = comp_data.drop(\n",
    "        columns=['accepts_mercadopago',\n",
    "                # 'available_quantity',\n",
    "                # 'avg_gmv_item_domain_30days',\n",
    "                # 'avg_gmv_item_sel',\n",
    "                # 'avg_gmv_seller_bday',\n",
    "                # 'avg_qty_orders_item_domain_30days',\n",
    "                # 'avg_qty_orders_item_sel_30days',\n",
    "                # 'avg_si_item_sel_30day',\n",
    "                'benefit',\n",
    "                'boosted',\n",
    "                # 'category_id',\n",
    "                # 'conversion',\n",
    "                'date',\n",
    "                'deal_print_id',\n",
    "                # 'domain_id',\n",
    "                'etl_version',\n",
    "                # 'free_shipping',\n",
    "                # 'fulfillment',\n",
    "                'full_name',\n",
    "                # 'health',\n",
    "                'is_pdp',\n",
    "                # 'product_id',\n",
    "                # 'item_id',\n",
    "                # 'listing_type_id',\n",
    "                # 'logistic_type',\n",
    "                # 'main_picture',\n",
    "                # 'offset',\n",
    "                # 'original_price',\n",
    "                # 'platform',\n",
    "                # 'price',\n",
    "                # 'print_position',\n",
    "                'print_server_timestamp',\n",
    "                # 'qty_items_dom',\n",
    "                # 'qty_items_sel',\n",
    "                # 'site_id',\n",
    "                # 'sold_quantity',\n",
    "                'tags',\n",
    "                'title',\n",
    "                # 'total_asp_item_domain_30days',\n",
    "                # 'total_asp_item_sel_30days',\n",
    "                # 'total_gmv_domain_bday',\n",
    "                # 'total_gmv_item_30days',\n",
    "                # 'total_items_domain',\n",
    "                # 'total_items_seller',\n",
    "                # 'total_orders_domain_30days',\n",
    "                # 'total_orders_item_30days',\n",
    "                # 'total_orders_sel_30days',\n",
    "                # 'total_si_domain_30days',\n",
    "                # 'total_si_item_30days',\n",
    "                # 'total_si_sel_30days',\n",
    "                # 'total_visits_domain',\n",
    "                # 'total_visits_item',\n",
    "                # 'total_visits_seller',\n",
    "                'uid',\n",
    "                'user_id',\n",
    "                'warranty',\n",
    "                # 'ROW_ID'\n",
    "                ]\n",
    "    )\n",
    "    # OHE comp_data columns\n",
    "    comp_data = pd.get_dummies(comp_data, \n",
    "        sparse=True,\n",
    "        columns=[\n",
    "            #\"category_id\", \n",
    "            #\"domain_id\", \n",
    "            \"logistic_type\", \n",
    "            \"platform\", \n",
    "            \"site_id\"\n",
    "        ],\n",
    "        dtype=int\n",
    "    )\n",
    "    #comp_data[\"accepts_mercadopago\"] = comp_data[\"accepts_mercadopago\"].astype(int)\n",
    "    # todos aceptan mercadopago\n",
    "    # comp_data[\"boosted\"] = comp_data[\"boosted\"].astype(int)\n",
    "    comp_data[\"free_shipping\"] = comp_data[\"free_shipping\"].astype(int)\n",
    "    comp_data[\"fulfillment\"] = comp_data[\"fulfillment\"].astype(int)\n",
    "\n",
    "    # comp_data[\"is_pdp\"].fillna(0, inplace=True)\n",
    "    # comp_data[\"is_pdp\"] = comp_data[\"is_pdp\"].astype(int)\n",
    "    #comp_data[\"warranty\"] = comp_data[\"warranty\"].astype(int)\n",
    "\n",
    "    # comp_data[\"listing_type_id\"] to 0 if gold_special, 1 if gold_pro.\n",
    "    comp_data[\"listing_type_id\"] = comp_data[\"listing_type_id\"].apply(lambda x: 0 if x == \"gold_special\" else 1)\n",
    "    \n",
    "    # Label encode category_id and domain_id\n",
    "    # comp_data[\"category_id\"] = comp_data[\"category_id\"].astype(\"category\")#.cat.codes\n",
    "    #comp_data[\"domain_id\"] = comp_data[\"domain_id\"].astype(\"category\")#.cat.codes\n",
    "\n",
    "    # sklearn LabelEncoder for category_id and domain_id\n",
    "    \n",
    "    comp_data[\"category_id\"] = LabelEncoder().fit_transform(comp_data[\"category_id\"]).astype(int)\n",
    "    comp_data[\"domain_id\"] = LabelEncoder().fit_transform(comp_data[\"domain_id\"]).astype(int)\n",
    "    comp_data[\"item_id\"] = LabelEncoder().fit_transform(comp_data[\"item_id\"]).astype(int)\n",
    "\n",
    "    comp_data[\"price_diff\"] = comp_data[\"price\"] - comp_data[\"original_price\"]\n",
    "    comp_data[\"cheaper_than_original\"] = comp_data[\"price_diff\"].apply(lambda x: 1 if x < 0 else 0)\n",
    "    comp_data[\"price_diff\"] = comp_data[\"price_diff\"].apply(lambda x: abs(x)).astype(int)\n",
    "\n",
    "    #comp_data[\"cheaper_than_avg\"] = comp_data[\"price\"] - comp_data[\"avg_asp_item_domain\"]\n",
    "    #comp_data[\"cheaper_than_avg\"] = comp_data[\"cheaper_than_avg\"].apply(lambda x: 1 if x < 0 else 0).astype(int)\n",
    "\n",
    "    # Drop useless columns\n",
    "    # comp_data = comp_data.drop(\n",
    "    #     columns=[\n",
    "    #     ]\n",
    "    # )\n",
    "\n",
    "    return comp_data\n",
    "comp_data = load_comp()\n",
    "comp_data[\"item_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLIT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **available_quantity** (int64)\n",
       "- **avg_gmv_item_domain_30days** (float64)\n",
       "- **avg_gmv_item_sel** (float64)\n",
       "- **avg_gmv_seller_bday** (float64)\n",
       "- **avg_qty_orders_item_domain_30days** (float64)\n",
       "- **avg_qty_orders_item_sel_30days** (float64)\n",
       "- **avg_si_item_sel_30day** (float64)\n",
       "- **category_id** (int32)\n",
       "- **conversion** (float64)\n",
       "- **domain_id** (int32)\n",
       "- **free_shipping** (int32)\n",
       "- **fulfillment** (int32)\n",
       "- **health** (float64)\n",
       "- **product_id** (float64)\n",
       "- **item_id** (int32)\n",
       "- **listing_type_id** (int64)\n",
       "- **main_picture** (object)\n",
       "- **offset** (int64)\n",
       "- **original_price** (int64)\n",
       "- **price** (int64)\n",
       "- **print_position** (int64)\n",
       "- **qty_items_dom** (float64)\n",
       "- **qty_items_sel** (float64)\n",
       "- **sold_quantity** (int64)\n",
       "- **total_asp_item_domain_30days** (float64)\n",
       "- **total_asp_item_sel_30days** (float64)\n",
       "- **total_gmv_domain_bday** (float64)\n",
       "- **total_gmv_item_30days** (float64)\n",
       "- **total_items_domain** (int64)\n",
       "- **total_items_seller** (int64)\n",
       "- **total_orders_domain_30days** (float64)\n",
       "- **total_orders_item_30days** (float64)\n",
       "- **total_orders_sel_30days** (float64)\n",
       "- **total_si_domain_30days** (float64)\n",
       "- **total_si_item_30days** (float64)\n",
       "- **total_si_sel_30days** (float64)\n",
       "- **total_visits_domain** (int64)\n",
       "- **total_visits_item** (int64)\n",
       "- **total_visits_seller** (int64)\n",
       "- **ROW_ID** (float64)\n",
       "- **logistic_type_cross_docking** (Sparse[int32, 0])\n",
       "- **logistic_type_custom** (Sparse[int32, 0])\n",
       "- **logistic_type_default** (Sparse[int32, 0])\n",
       "- **logistic_type_drop_off** (Sparse[int32, 0])\n",
       "- **logistic_type_fulfillment** (Sparse[int32, 0])\n",
       "- **logistic_type_not_specified** (Sparse[int32, 0])\n",
       "- **logistic_type_xd_drop_off** (Sparse[int32, 0])\n",
       "- **platform_/mobile/android** (Sparse[int32, 0])\n",
       "- **platform_/mobile/ios** (Sparse[int32, 0])\n",
       "- **platform_/web/desktop** (Sparse[int32, 0])\n",
       "- **platform_/web/mobile** (Sparse[int32, 0])\n",
       "- **site_id_MLA** (Sparse[int32, 0])\n",
       "- **price_diff** (int32)\n",
       "- **cheaper_than_original** (int64)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into training and evaluation samples\n",
    "comp_data = load_comp()\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        \"\\n\".join(\n",
    "            [\n",
    "                \"- **{}** ({})\".format(col, dtype) for col, dtype in\n",
    "                zip(comp_data.columns, comp_data.dtypes)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "full_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "\n",
    "# Get number column names for full_data\n",
    "#print(full_data.columns)\n",
    "#num_cols = full_data.select_dtypes(include='number').columns\n",
    "#print(num_cols)\n",
    "# difference between full_data_cols and num_cols\n",
    "# print(set(full_data.columns) - set(num_cols))\n",
    "\n",
    "del comp_data\n",
    "gc.collect()\n",
    "\n",
    "train_data, test_data = train_test_split(full_data, test_size=0.20, train_size=0.80, random_state=42)\n",
    "\n",
    "y_train = train_data[\"conversion\"]\n",
    "X_train = train_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "X_train = X_train.select_dtypes(include='number')\n",
    "\n",
    "y_test = test_data[\"conversion\"]\n",
    "X_test = test_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "X_test = X_test.select_dtypes(include='number')\n",
    "\n",
    "del train_data\n",
    "del test_data\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\oltie\\OneDrive\\Escritorio\\DiTella\\6to_Semestre_23\\TD6\\TP2\\tp2_td6\\experimentacion_oli.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 88>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#W6sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m X_train_kf, X_valid_kf \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39miloc[train_index], X_train\u001b[39m.\u001b[39miloc[valid_index]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#W6sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m y_train_kf, y_valid_kf \u001b[39m=\u001b[39m y_train\u001b[39m.\u001b[39miloc[train_index], y_train\u001b[39m.\u001b[39miloc[valid_index]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#W6sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m dtc\u001b[39m.\u001b[39;49mfit(X_train_kf, y_train_kf)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#W6sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39m# Metric: roc_auc_score\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#W6sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m roc_aucs\u001b[39m.\u001b[39mappend(roc_auc_score(y_valid_kf, dtc\u001b[39m.\u001b[39mpredict_proba(X_valid_kf)[:, dtc\u001b[39m.\u001b[39mclasses_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    419\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 420\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    422\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1806\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1805\u001b[0m     \u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1806\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1807\u001b[0m         ParameterSampler(\n\u001b[0;32m   1808\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1809\u001b[0m         )\n\u001b[0;32m   1810\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "atributes_to_poly = [\n",
    "                        \"available_quantity\", \n",
    "                        \"avg_gmv_item_domain_30days\", \n",
    "                        \"avg_gmv_item_sel\", \n",
    "                        \"avg_gmv_seller_bday\", \n",
    "                        \"avg_qty_orders_item_domain_30days\", \n",
    "                        \"avg_qty_orders_item_sel_30days\", \n",
    "                        \"avg_si_item_sel_30day\",\n",
    "                        \"original_price\",\n",
    "                        \"price\",\n",
    "                    ]\n",
    "\n",
    "imputed_poly_features = make_pipeline(\n",
    "  #SimpleImputer(),\n",
    "  #PolynomialFeatures(),\n",
    "  StandardScaler(),\n",
    "  xgb.XGBClassifier(\n",
    "        missing=np.nan,\n",
    "        n_jobs=-1,\n",
    "        tree_method='gpu_hist',\n",
    "    )\n",
    ")\n",
    "\n",
    "# Custom Layer Starts Here:\n",
    "pl = PolynomialFeatures(2, interaction_only=True, include_bias=False)\n",
    "\n",
    "#Input X contains NaN.\n",
    "\n",
    "imputed_X_train_plus = X_train.copy()\n",
    "imputed_X_test_plus = X_test.copy()\n",
    "print(imputed_X_train_plus.shape[0] == X_train.shape[0])\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "cols_with_missing = (col for col in X_train.columns\n",
    "                        if X_train[col].isnull().any())\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n",
    "    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "poly_X_train = imputed_X_train_plus[atributes_to_poly]\n",
    "poly_X_test = imputed_X_test_plus[atributes_to_poly]\n",
    "print(poly_X_train.shape[0] == X_train.shape[0])\n",
    "\n",
    "poly_X_train = my_imputer.fit_transform(poly_X_train)\n",
    "poly_X_test = my_imputer.transform(poly_X_test)\n",
    "print(poly_X_train.shape[0] == X_train.shape[0])\n",
    "\n",
    "poly_X_train = pl.fit_transform(poly_X_train)\n",
    "poly_X_test = pl.fit_transform(poly_X_test)\n",
    "print(poly_X_train.shape[0] == X_train.shape[0])\n",
    "\n",
    "new_X_train = np.concatenate((X_train.drop(atributes_to_poly, axis=1), poly_X_train), axis=1)\n",
    "new_X_test = np.concatenate((X_test.drop(atributes_to_poly, axis=1), poly_X_test), axis=1)\n",
    "print(new_X_train.shape[0] == X_train.shape[0])\n",
    "# Custom Layer Ends Here\n",
    "\n",
    "\n",
    "imputed_poly_features.fit(new_X_train, y_train)\n",
    "\n",
    "roc_auc_score(y_test, imputed_poly_features.predict_proba(new_X_test)[:, imputed_poly_features.classes_ == 1])\n",
    "# Decision Tree Classifier with K-Fold Cross Validation and Randomized Search CV\n",
    "\n",
    "dtc = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    RandomizedSearchCV(\n",
    "        DecisionTreeClassifier(random_state=2345),\n",
    "        param_distributions={\n",
    "            \"max_depth\": [2, 4, 8, 16, 32, 64, 128, 256, 512, None],\n",
    "            \"min_samples_split\": [2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "            \"min_samples_leaf\": [2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "            \"max_features\": [2, 4, 8, 16, 32, 64, 128, 256, 512, None],\n",
    "        },\n",
    "        n_iter=100,\n",
    "        cv=3,\n",
    "        random_state=2345,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=2345, shuffle=True)\n",
    "roc_aucs = []\n",
    "\n",
    "for train_index, valid_index in kf.split(X_train):\n",
    "    X_train_kf, X_valid_kf = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "    y_train_kf, y_valid_kf = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "\n",
    "    dtc.fit(X_train_kf, y_train_kf)\n",
    "    # Metric: roc_auc_score\n",
    "    roc_aucs.append(roc_auc_score(y_valid_kf, dtc.predict_proba(X_valid_kf)[:, dtc.classes_ == 1]))\n",
    "    print(f\"ROC-AUC split {len(roc_aucs)}: \", roc_aucs[-1])\n",
    "\n",
    "print(\"ROC-AUC mean: \", sum(roc_aucs) / len(roc_aucs))    \n",
    "\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "print(\"ROC-AUC: \", roc_auc_score(y_test, dtc.predict_proba(X_test)[:, dtc.classes_ == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\oltie\\OneDrive\\Escritorio\\DiTella\\6to_Semestre_23\\TD6\\TP2\\tp2_td6\\experimentacion_oli.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m submission_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m\"\u001b[39m\u001b[39mROW_ID\u001b[39m\u001b[39m\"\u001b[39m: eval_data[\u001b[39m\"\u001b[39m\u001b[39mROW_ID\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mconversion\u001b[39m\u001b[39m\"\u001b[39m: y_preds})\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m submission_df[\u001b[39m\"\u001b[39m\u001b[39mROW_ID\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m submission_df[\u001b[39m\"\u001b[39m\u001b[39mROW_ID\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m submission_df\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mexp_oli_1.csv\u001b[39m\u001b[39m\"\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_preds' is not defined"
     ]
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({\"ROW_ID\": eval_data[\"ROW_ID\"], \"conversion\": y_preds})\n",
    "submission_df[\"ROW_ID\"] = submission_df[\"ROW_ID\"].astype(int)\n",
    "submission_df.to_csv(\"exp_oli_1.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------\n",
    "# Intento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data = pd.read_csv(\"../competition_data.csv\")\n",
    "\n",
    "# Split into training and evaluation samples\n",
    "train_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "del comp_data\n",
    "gc.collect()\n",
    "\n",
    "# Train a random forest model on the train data\n",
    "train_data = train_data.sample(frac=1/3)\n",
    "y_train = train_data[\"conversion\"]\n",
    "X_train = train_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "X_train = X_train.select_dtypes(include='number')\n",
    "del train_data\n",
    "gc.collect()\n",
    "\n",
    "cls = make_pipeline(SimpleImputer(), DecisionTreeClassifier(max_depth=8, random_state=2345))\n",
    "cls.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the evaluation set\n",
    "eval_data = eval_data.drop(columns=[\"conversion\"])\n",
    "eval_data = eval_data.select_dtypes(include='number')\n",
    "y_preds = cls.predict_proba(eval_data.drop(columns=[\"ROW_ID\"]))[:, cls.classes_ == 1].squeeze()\n",
    "\n",
    "# Make the submission file\n",
    "submission_df = pd.DataFrame({\"ROW_ID\": eval_data[\"ROW_ID\"], \"conversion\": y_preds})\n",
    "submission_df[\"ROW_ID\"] = submission_df[\"ROW_ID\"].astype(int)\n",
    "submission_df.to_csv(\"basic_model_oli.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "comp_data = pd.read_csv(\"../competition_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This SimpleImputer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\oltie\\OneDrive\\Escritorio\\DiTella\\6to_Semestre_23\\TD6\\TP2\\tp2_td6\\experimentacion_oli.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#X41sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m eval_data \u001b[39m=\u001b[39m eval_data\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mconversion\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#X41sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m eval_data \u001b[39m=\u001b[39m eval_data\u001b[39m.\u001b[39mselect_dtypes(include\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumber\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#X41sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m y_preds \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_proba(eval_data\u001b[39m.\u001b[39;49mdrop(columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mROW_ID\u001b[39;49m\u001b[39m\"\u001b[39;49m]))[:, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#X41sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m kf \u001b[39m=\u001b[39m KFold(n_splits\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m2345\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#X41sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m roc_aucs \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:577\u001b[0m, in \u001b[0;36mPipeline.predict_proba\u001b[1;34m(self, X, **predict_proba_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m Xt \u001b[39m=\u001b[39m X\n\u001b[0;32m    576\u001b[0m \u001b[39mfor\u001b[39;00m _, name, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(with_final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 577\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(Xt)\n\u001b[0;32m    578\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpredict_proba(Xt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpredict_proba_params)\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\impute\\_base.py:525\u001b[0m, in \u001b[0;36mSimpleImputer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    512\u001b[0m     \u001b[39m\"\"\"Impute all missing values in `X`.\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \n\u001b[0;32m    514\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[39m        `X` with imputed values.\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 525\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    527\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_input(X, in_fit\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    528\u001b[0m     statistics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatistics_\n",
      "File \u001b[1;32mc:\\Users\\oltie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1462\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not an estimator instance.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (estimator))\n\u001b[0;32m   1461\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1462\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(msg \u001b[39m%\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(estimator)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This SimpleImputer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "#data cleaning\n",
    "comp_data = comp_data.drop(\n",
    "        columns=['accepts_mercadopago',\n",
    "                # 'available_quantity',\n",
    "                # 'avg_gmv_item_domain_30days',\n",
    "                # 'avg_gmv_item_sel',\n",
    "                # 'avg_gmv_seller_bday',\n",
    "                # 'avg_qty_orders_item_domain_30days',\n",
    "                # 'avg_qty_orders_item_sel_30days',\n",
    "                # 'avg_si_item_sel_30day',\n",
    "                'benefit',\n",
    "                'boosted',\n",
    "                # 'category_id',\n",
    "                # 'conversion',\n",
    "                'date',\n",
    "                'deal_print_id',\n",
    "                # 'domain_id',\n",
    "                'etl_version',\n",
    "                # 'free_shipping',\n",
    "                # 'fulfillment',\n",
    "                'full_name',\n",
    "                # 'health',\n",
    "                'is_pdp',\n",
    "                # 'product_id',\n",
    "                # 'item_id',\n",
    "                # 'listing_type_id',\n",
    "                # 'logistic_type',\n",
    "                # 'main_picture',\n",
    "                # 'offset',\n",
    "                # 'original_price',\n",
    "                # 'platform',\n",
    "                # 'price',\n",
    "                # 'print_position',\n",
    "                'print_server_timestamp',\n",
    "                # 'qty_items_dom',\n",
    "                # 'qty_items_sel',\n",
    "                # 'site_id',\n",
    "                # 'sold_quantity',\n",
    "                'tags',\n",
    "                'title',\n",
    "                # 'total_asp_item_domain_30days',\n",
    "                # 'total_asp_item_sel_30days',\n",
    "                # 'total_gmv_domain_bday',\n",
    "                # 'total_gmv_item_30days',\n",
    "                # 'total_items_domain',\n",
    "                # 'total_items_seller',\n",
    "                # 'total_orders_domain_30days',\n",
    "                # 'total_orders_item_30days',\n",
    "                # 'total_orders_sel_30days',\n",
    "                # 'total_si_domain_30days',\n",
    "                # 'total_si_item_30days',\n",
    "                # 'total_si_sel_30days',\n",
    "                # 'total_visits_domain',\n",
    "                # 'total_visits_item',\n",
    "                # 'total_visits_seller',\n",
    "                'uid',\n",
    "                'user_id',\n",
    "                'warranty',\n",
    "                # 'ROW_ID'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "#OHE comp_data columns\n",
    "comp_data = pd.get_dummies(comp_data,\n",
    "    sparse=True)\n",
    "\n",
    "train_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "del comp_data\n",
    "gc.collect()\n",
    "\n",
    "# Train a random forest model on the train data\n",
    "train_data = train_data.sample(frac=1/3)\n",
    "y_train = train_data[\"conversion\"]\n",
    "X_train = train_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "X_train = X_train.select_dtypes(include='number')\n",
    "del train_data\n",
    "gc.collect()\n",
    "\n",
    "cls = make_pipeline(SimpleImputer(), DecisionTreeClassifier(max_depth=8, random_state=2345))\n",
    "cls.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the evaluation set\n",
    "eval_data = eval_data.drop(columns=[\"conversion\"])\n",
    "eval_data = eval_data.select_dtypes(include='number')\n",
    "y_preds = cls.predict_proba(eval_data.drop(columns=[\"ROW_ID\"]))[:, cls.classes_ == 1].squeeze()\n",
    "\n",
    "\n",
    "# kf = KFold(n_splits=5, random_state=2345, shuffle=True)\n",
    "# roc_aucs = []\n",
    "\n",
    "# dtc = make_pipeline(\n",
    "#     SimpleImputer(),\n",
    "#     RandomizedSearchCV(\n",
    "#         DecisionTreeClassifier(random_state=2345),\n",
    "#         param_distributions={\n",
    "#             \"max_depth\": [2, 4, 8, 16, 32, 64, 128, 256, 512, None],\n",
    "#             \"min_samples_split\": [2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "#             \"min_samples_leaf\": [2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "#             \"max_features\": [2, 4, 8, 16, 32, 64, 128, 256, 512, None],\n",
    "#         },\n",
    "#         n_iter=100,\n",
    "#         cv=3,\n",
    "#         random_state=2345,\n",
    "#         n_jobs=-1,\n",
    "#         verbose=0,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# for train_index, valid_index in kf.split(X_train):\n",
    "#     X_train_kf, X_valid_kf = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "#     y_train_kf, y_valid_kf = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "\n",
    "#     dtc.fit(X_train_kf, y_train_kf)\n",
    "#     # Metric: roc_auc_score\n",
    "#     roc_aucs.append(roc_auc_score(y_valid_kf, dtc.predict_proba(X_valid_kf)[:, dtc.classes_ == 1]))\n",
    "#     print(f\"ROC-AUC split {len(roc_aucs)}: \", roc_aucs[-1])\n",
    "\n",
    "# print(\"ROC-AUC mean: \", sum(roc_aucs) / len(roc_aucs))    \n",
    "\n",
    "# dtc.fit(X_train, y_train)\n",
    "\n",
    "# print(\"ROC-AUC: \", roc_auc_score(y_test, dtc.predict_proba(X_test)[:, dtc.classes_ == 1]))\n",
    "\n",
    "# Make the submission file\n",
    "submission_df = pd.DataFrame({\"ROW_ID\": eval_data[\"ROW_ID\"], \"conversion\": y_preds})\n",
    "submission_df[\"ROW_ID\"] = submission_df[\"ROW_ID\"].astype(int)\n",
    "submission_df.to_csv(\"basic_model_oli.csv\", sep=\",\", index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------\n",
    "# CODIGO AYUDA \n",
    "### KFOLD CLASE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross validation\n",
    "kcv = KFold(n_splits=10, shuffle=True)\n",
    "rmse_kcv1 = []\n",
    "for train_ix, test_ix in tqdm(kcv.split(X_train), total=10):\n",
    "    X_train_red, X_val = X_train.iloc[train_ix,:], X_train.iloc[test_ix,:]\n",
    "    y_train_red, y_val = y_train[train_ix], y_train[test_ix]\n",
    "    tree = DecisionTreeRegressor(max_depth=10, random_state=TREE_SEED)\n",
    "    tree.fit(X_train_red, y_train_red)\n",
    "    preds_val = tree.predict(X_val)\n",
    "    rmse_kcv1.append(math.sqrt(mean_squared_error(y_val, preds_val)))\n",
    "\n",
    "print(f\"RMSE estimado mediante k-fold CV (VersiÃ³n 1): {mean(rmse_kcv1):.2f}\")\n",
    "\n",
    "# ValidaciÃ³n cruzada k-fold utilizando cross_val_score\n",
    "kcv = KFold(n_splits=10, shuffle=True)\n",
    "tree = DecisionTreeRegressor(max_depth=10, random_state=TREE_SEED)\n",
    "rmse_kcv2 = cross_val_score(tree, X_train, y_train, scoring='neg_root_mean_squared_error', cv=kcv, n_jobs=-1)\n",
    "\n",
    "print(f\"RMSE estimado mediante k-fold CV (VersiÃ³n 2): {-1 * mean(rmse_kcv2):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "no se si usar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, holdout y testing sets\n",
    "size_val = math.ceil(0.1 * X_train.shape[0])\n",
    "size_test = math.ceil(0.1 * X_train.shape[0])\n",
    "\n",
    "X_train_red, X_test, y_train_red, y_test = train_test_split(X_train, y_train, test_size=size_test)\n",
    "X_train_red, X_val, y_train_red, y_val = train_test_split(X_train_red, y_train_red, test_size=size_val)\n",
    "\n",
    "# Prueba con distintos niveles de profundidad (model selection)\n",
    "exp_results = []\n",
    "for md in tqdm(range(1, 51)):\n",
    "    tree = DecisionTreeRegressor(max_depth=md, random_state=TREE_SEED)\n",
    "    tree.fit(X_train_red, y_train_red)\n",
    "    preds_val = tree.predict(X_val)\n",
    "    exp_results.append({\"max_depth\": md,\n",
    "                        \"rmse_val\": math.sqrt(mean_squared_error(y_val, preds_val))})\n",
    "\n",
    "exp_results = pd.DataFrame(exp_results)\n",
    "plot_exp(exp_results)\n",
    "\n",
    "# Se entrena el Ã¡rbol con la mejor profundidad encontrada sobre train set + validation set\n",
    "best_md = exp_results[exp_results[\"rmse_val\"].min() == exp_results[\"rmse_val\"]]\n",
    "best_md = best_md.sort_values(\"max_depth\").iloc[0,:]\n",
    "print(f\"Performance del mejor modelos: {best_md['rmse_val']:.2f}\")\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=int(best_md[\"max_depth\"]), random_state=TREE_SEED)\n",
    "tree.fit(pd.concat([X_train_red, X_val], axis=0),\n",
    "         pd.concat([y_train_red, y_val], axis=0))\n",
    "preds_test = tree.predict(X_test)\n",
    "\n",
    "print(f\"RMSE estimado en test: {math.sqrt(mean_squared_error(y_test, preds_test)):.2f}\")\n",
    "\n",
    "# EvaluaciÃ³n final en el conjunto de evaluaciÃ³n (model assestment)\n",
    "tree = DecisionTreeRegressor(max_depth=int(best_md[\"max_depth\"]), random_state=TREE_SEED)\n",
    "tree.fit(X_train, y_train)\n",
    "preds_eval = tree.predict(X_eval)\n",
    "_, _, _, y_eval = load_data()\n",
    "\n",
    "print(f\"Performance de evaluaciÃ³n: {math.sqrt(mean_squared_error(y_eval, preds_eval)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hstack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\oltie\\OneDrive\\Escritorio\\DiTella\\6to_Semestre_23\\TD6\\TP2\\tp2_td6\\experimentacion_oli.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m encoder \u001b[39m=\u001b[39m OneHotEncoder(sparse_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m X_train_ohe_obj \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39mfit_transform(X_train_obj)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m X_train_ohe_v2 \u001b[39m=\u001b[39m hstack([X_train_ohe_obj, scaler\u001b[39m.\u001b[39mfit_transform(X_train_num)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Sobre validation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oltie/OneDrive/Escritorio/DiTella/6to_Semestre_23/TD6/TP2/tp2_td6/experimentacion_oli.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m X_val_obj \u001b[39m=\u001b[39m X_val\u001b[39m.\u001b[39mselect_dtypes(include\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hstack' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, precision_recall_fscore_support, precision_recall_curve, average_precision_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# One-hot-encoding\n",
    "# OpciÃ³n 1: utilizar get_dummies de pandas\n",
    "X_train_ohe_v1 = pd.get_dummies(X_train).head()  # Y ahora cÃ³mo lo aplico en validaciÃ³n? (deberÃ­a unir antes los datos)\n",
    "\n",
    "# OpciÃ³n 2: utilizar onehotencoder de sklearn (mÃ¡s conveniente para producciÃ³n)\n",
    "\n",
    "# Sobre training\n",
    "X_train_obj = X_train.select_dtypes(include=[\"object\"])\n",
    "X_train_num = X_train.select_dtypes(include=[\"int\", \"float\"])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder(sparse_output=True)\n",
    "X_train_ohe_obj = encoder.fit_transform(X_train_obj)\n",
    "X_train_ohe_v2 = hstack([X_train_ohe_obj, scaler.fit_transform(X_train_num)])\n",
    "\n",
    "# Sobre validation\n",
    "X_val_obj = X_val.select_dtypes(include=[\"object\"])\n",
    "X_val_num = X_val.select_dtypes(include=[\"int\", \"float\"])\n",
    "\n",
    "X_val_ohe_obj = encoder.transform(X_val_obj)\n",
    "X_val_ohe_v2 = hstack([X_val_ohe_obj, scaler.transform(X_val_num)])\n",
    "\n",
    "# Entrenamos un modelo de regresiÃ³n logÃ­stica sobre los datos con OHE sin escalado\n",
    "lr_exp = []\n",
    "for C in tqdm([0.001, 0.0025, 0.01, 0.05, 0.075, 0.1, 0.25]):\n",
    "    lr = LogisticRegression(C=C, max_iter=2000)\n",
    "    lr.fit(X_train_ohe_v2, y_train)\n",
    "    preds = lr.predict_proba(X_val_ohe_v2)\n",
    "    preds = preds[:, lr.classes_ == \"yes\"]\n",
    "    auc = roc_auc_score(y_val == \"yes\", preds)\n",
    "    lr_exp.append({\"C\": C, \"auc_val\": auc})\n",
    "\n",
    "lr_exp = pd.DataFrame(lr_exp)\n",
    "print(lr_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

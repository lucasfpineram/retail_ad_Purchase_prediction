{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas Generales\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import tqdm\n",
    "import logging\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "# K-Fold y Regresión\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from statistics import mean, stdev\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import LeaveOneOut, KFold, cross_val_score\n",
    "\n",
    "# Hyperopt\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "from sklearn.metrics import balanced_accuracy_score, make_scorer, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# W2V (Word2Vec)\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configuración\n",
    "random_state = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargo los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the competition data\n",
    "comp_data = pd.read_csv(\"../competition_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuneo de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retoque de fecha - puedo ahcerlo antes porque son pocas y oredecibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data['date'] = pd.to_datetime(comp_data['date'])\n",
    "comp_data['month'] = comp_data['date'].dt.month\n",
    "# comp_data['day'] = comp_data['date'].dt.day\n",
    "comp_data['dayofweek'] = comp_data['date'].dt.dayofweek\n",
    "# comp_data['hour'] = comp_data['date'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data['is_pdp'].fillna(-1, inplace=True)\n",
    "comp_data['is_pdp'] = comp_data['is_pdp'].replace({'True': 1, 'False': 0})\n",
    "comp_data['is_pdp'] = comp_data['is_pdp'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data[\"warranty\"] = comp_data[\"warranty\"].str.lower()\n",
    "comp_data[\"warranty\"] = comp_data[\"warranty\"].str.replace(\"á\", \"a\")\n",
    "comp_data[\"warranty\"] = comp_data[\"warranty\"].str.replace(\"í\", \"i\")\n",
    "\n",
    "def has_warranty(value):\n",
    "    if value == 'Sin garantia':\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "comp_data['has_warranty'] = comp_data['warranty'].apply(has_warranty)\n",
    "comp_data[\"warranty_missing\"] = (~comp_data[\"warranty\"].isna()).astype(float)\n",
    "comp_data[\"warranty_vendedor\"] = comp_data[\"warranty\"].str.contains(\"vendedor\").astype(float)\n",
    "comp_data[\"warranty_fabrica\"] = comp_data[\"warranty\"].str.contains(\"fabrica\").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_tags = True\n",
    "\n",
    "tags = []\n",
    "names = []\n",
    "\n",
    "if add_tags:\n",
    "    if len(tags) == 0:\n",
    "        #tags = comp_data[\"tags\"].str.replace(\"[\", \"\").str.replace(\"]\", \"\").str.split(\", \").apply(pd.Series).stack().value_counts()\n",
    "        # more efficient\n",
    "        tags = comp_data[\"tags\"].str.replace(\"[\", \"\").str.replace(\"]\", \"\").str.split(\", \").explode().value_counts()\n",
    "\n",
    "    for tag in tags.index:\n",
    "        comp_data[\"tag_\" + tag] = comp_data[\"tags\"].str.contains(tag).astype(int)\n",
    "\n",
    "    comp_data[\"tags_count\"] = comp_data[\"tags\"].str.replace(\"[\", \"\").str.replace(\"]\", \"\").str.split(\", \").apply(len)\n",
    "\n",
    "    # comp_data = comp_data.drop(\"tags\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data['pdp_price'] = comp_data['is_pdp'] * comp_data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_attrs = [\"print_position\", \"offset\", \"price\", \"health\", \"original_price\"]\n",
    "\n",
    "for x in poly_attrs:\n",
    "    comp_data[x + \"2\"] = comp_data[x] ** 2\n",
    "\n",
    "for (x, y) in itertools.combinations(poly_attrs, 2):\n",
    "    comp_data[x + \"2 + \" + y + \"2\"] = comp_data[x] ** 2 + comp_data[y] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2Vec \n",
    "en title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if w2v:\n",
    "    # Descarga de stopwords para español si no están descargadas ya\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    STOP_WORDS_SP = set(stopwords.words('spanish'))\n",
    "\n",
    "    def iterate_LN_corpus(path):\n",
    "        \"\"\"\n",
    "        Genera un iterador para recorrer los archivos de texto en un directorio.\n",
    "\n",
    "        Args:\n",
    "            path (str): Ruta al directorio que contiene los archivos.\n",
    "\n",
    "        Yields:\n",
    "            str: Texto contenido en cada archivo.\n",
    "        \"\"\"\n",
    "        articles = os.listdir(path)\n",
    "        random.shuffle(articles)\n",
    "        for art in articles:\n",
    "            with open(path + art, encoding=\"utf-8\") as f:\n",
    "                raw_text = f.read()\n",
    "            yield(raw_text)\n",
    "\n",
    "    def tokenizer(raw_text):\n",
    "        \"\"\"\n",
    "        Tokeniza y preprocesa un texto.\n",
    "\n",
    "        Args:\n",
    "            raw_text (str): Texto sin procesar.\n",
    "\n",
    "        Returns:\n",
    "            list: Lista de oraciones, donde cada oración es una lista de palabras.\n",
    "        \"\"\"\n",
    "        sentences = sent_tokenize(raw_text)\n",
    "        sentences = [word_tokenize(e) for e in sentences]\n",
    "        sentences = [[e2 for e2 in e1 if re.compile(\"[A-Za-z]\").search(e2[0])] for e1 in sentences]\n",
    "        sentences = [[e2.lower() for e2 in e1] for e1 in sentences]\n",
    "        print(sentences)\n",
    "        return(sentences)\n",
    "\n",
    "    def gen_sentences(path):\n",
    "        \"\"\"\n",
    "        Genera una lista de oraciones a partir de archivos de texto en un directorio.\n",
    "\n",
    "        Args:\n",
    "            path (str): Ruta al directorio que contiene los archivos de texto.\n",
    "\n",
    "        Returns:\n",
    "            list: Lista de oraciones.\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        n_arts = len(os.listdir(path))\n",
    "        for i, art in tqdm.tqdm(enumerate(iterate_LN_corpus(path)), total=n_arts):\n",
    "            sentences.extend(tokenizer(art))\n",
    "        return(sentences)\n",
    "\n",
    "    def average_vectors(title_tokens, model, stopwords=None):\n",
    "        \"\"\"\n",
    "        Calcula el vector promedio de un conjunto de tokens utilizando un modelo Word2Vec.\n",
    "\n",
    "        Args:\n",
    "            title_tokens (list): Lista de tokens.\n",
    "            model (gensim.models.Word2Vec): Modelo Word2Vec.\n",
    "            stopwords (set, optional): Conjunto de palabras stopwords. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Vector promedio.\n",
    "        \"\"\"\n",
    "        title_tokens = [e2 for e1 in title_tokens for e2 in e1]\n",
    "        title_tokens = [e for e in title_tokens if e in model.wv]\n",
    "        if stopwords is not None:\n",
    "            title_tokens = [e for e in title_tokens if e not in stopwords]\n",
    "        if len(title_tokens) == 0:\n",
    "            output = np.zeros(model.wv.vector_size)\n",
    "        else:\n",
    "            output = np.array([model.wv.get_vector(e) for e in title_tokens]).mean(0)\n",
    "        return output\n",
    "\n",
    "    def dummy_tokenizer(text_tokens):\n",
    "        \"\"\"\n",
    "        Tokenizador dummy que simplemente devuelve los tokens de texto sin procesar.\n",
    "\n",
    "        Args:\n",
    "            text_tokens (list): Lista de tokens.\n",
    "\n",
    "        Returns:\n",
    "            list: Misma lista de tokens de entrada.\n",
    "        \"\"\"\n",
    "        return text_tokens\n",
    "\n",
    "\n",
    "    #~ Análisis con datos de; TP ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    # Carga de datos desde un archivo CSV\n",
    "    # comp_data = pd.read_csv(\"competition_data.csv\")\n",
    "    comp_data[\"title_tokens\"] = comp_data[\"title\"].map(tokenizer)\n",
    "\n",
    "    # Creación del modelo Word2Vec\n",
    "    w2v_tp = gensim.models.Word2Vec(vector_size= 300,     # Tamaño del vector\n",
    "                                    window=5,             # Tamaño de ventana\n",
    "                                    min_count=10,         # Frecuencia mínima de palabra\n",
    "                                    negative=20,          # Número de ejemplos negativos para muestreo negativo\n",
    "                                    sample=0.001,         # Submuestreo de palabras frecuentes\n",
    "                                    workers=8,            # Número de núcleos de CPU para entrenamiento paralelo\n",
    "                                    sg=1)                 # Algoritmo: 1 para Skip-gram, 0 para CBOW\n",
    "\n",
    "    # Creación del vocabulario a partir del corpus\n",
    "    w2v_tp.build_vocab([e2 for e1 in comp_data[\"title_tokens\"].values for e2 in e1],\n",
    "                    progress_per=10000)\n",
    "\n",
    "    # Entrenamiento del modelo Word2Vec\n",
    "    w2v_tp.train([e2 for e1 in comp_data[\"title_tokens\"].values for e2 in e1],\n",
    "                total_examples=w2v_tp.corpus_count,\n",
    "                epochs=30, report_delay=1)\n",
    "\n",
    "    ## Ejercicio de predicción con word2vec\n",
    "\n",
    "    # Obtención de embeddings de títulos utilizando el modelo Word2Vec\n",
    "    title_embs = comp_data[\"title_tokens\"].map(lambda x: average_vectors(x, w2v_tp, STOP_WORDS_SP))\n",
    "    title_embs= np.array(title_embs.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v =False\n",
    "if w2v:\n",
    "    # Descarga de stopwords para español si no están descargadas ya\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    STOP_WORDS_SP = set(stopwords.words('spanish'))\n",
    "\n",
    "    def iterate_LN_corpus(path):\n",
    "        \"\"\"\n",
    "        Genera un iterador para recorrer los archivos de texto en un directorio.\n",
    "\n",
    "        Args:\n",
    "            path (str): Ruta al directorio que contiene los archivos.\n",
    "\n",
    "        Yields:\n",
    "            str: Texto contenido en cada archivo.\n",
    "        \"\"\"\n",
    "        articles = os.listdir(path)\n",
    "        random.shuffle(articles)\n",
    "        for art in articles:\n",
    "            with open(path + art, encoding=\"utf-8\") as f:\n",
    "                raw_text = f.read()\n",
    "            yield(raw_text)\n",
    "\n",
    "    def tokenizer(raw_text):\n",
    "        \"\"\"\n",
    "        Tokeniza y preprocesa un texto.\n",
    "\n",
    "        Args:\n",
    "            raw_text (str): Texto sin procesar.\n",
    "\n",
    "        Returns:\n",
    "            list: Lista de oraciones, donde cada oración es una lista de palabras.\n",
    "        \"\"\"\n",
    "        sentences = sent_tokenize(raw_text)\n",
    "        sentences = [word_tokenize(e) for e in sentences]\n",
    "        sentences = [[e2 for e2 in e1 if re.compile(\"[A-Za-z]\").search(e2[0])] for e1 in sentences]\n",
    "        sentences = [[e2.lower() for e2 in e1] for e1 in sentences]\n",
    "        print(sentences)\n",
    "        return(sentences)\n",
    "\n",
    "    def gen_sentences(path):\n",
    "        \"\"\"\n",
    "        Genera una lista de oraciones a partir de archivos de texto en un directorio.\n",
    "\n",
    "        Args:\n",
    "            path (str): Ruta al directorio que contiene los archivos de texto.\n",
    "\n",
    "        Returns:\n",
    "            list: Lista de oraciones.\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        n_arts = len(os.listdir(path))\n",
    "        for i, art in tqdm.tqdm(enumerate(iterate_LN_corpus(path)), total=n_arts):\n",
    "            sentences.extend(tokenizer(art))\n",
    "        return(sentences)\n",
    "\n",
    "    def average_vectors(title_tokens, model, stopwords=None):\n",
    "        \"\"\"\n",
    "        Calcula el vector promedio de un conjunto de tokens utilizando un modelo Word2Vec.\n",
    "\n",
    "        Args:\n",
    "            title_tokens (list): Lista de tokens.\n",
    "            model (gensim.models.Word2Vec): Modelo Word2Vec.\n",
    "            stopwords (set, optional): Conjunto de palabras stopwords. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Vector promedio.\n",
    "        \"\"\"\n",
    "        title_tokens = [e2 for e1 in title_tokens for e2 in e1]\n",
    "        title_tokens = [e for e in title_tokens if e in model.wv]\n",
    "        if stopwords is not None:\n",
    "            title_tokens = [e for e in title_tokens if e not in stopwords]\n",
    "        if len(title_tokens) == 0:\n",
    "            output = np.zeros(model.wv.vector_size)\n",
    "        else:\n",
    "            output = np.array([model.wv.get_vector(e) for e in title_tokens]).mean(0)\n",
    "        return output\n",
    "\n",
    "    def dummy_tokenizer(text_tokens):\n",
    "        \"\"\"\n",
    "        Tokenizador dummy que simplemente devuelve los tokens de texto sin procesar.\n",
    "\n",
    "        Args:\n",
    "            text_tokens (list): Lista de tokens.\n",
    "\n",
    "        Returns:\n",
    "            list: Misma lista de tokens de entrada.\n",
    "        \"\"\"\n",
    "        return text_tokens\n",
    "\n",
    "\n",
    "    #~ Análisis con datos de; TP ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    # Carga de datos desde un archivo CSV\n",
    "    # comp_data = pd.read_csv(\"competition_data.csv\")\n",
    "    comp_data[\"title_tokens\"] = comp_data[\"title\"].map(tokenizer)\n",
    "\n",
    "    # Creación del modelo Word2Vec\n",
    "    w2v_tp = gensim.models.Word2Vec(vector_size= 300,     # Tamaño del vector\n",
    "                                    window=5,             # Tamaño de ventana\n",
    "                                    min_count=10,         # Frecuencia mínima de palabra\n",
    "                                    negative=20,          # Número de ejemplos negativos para muestreo negativo\n",
    "                                    sample=0.001,         # Submuestreo de palabras frecuentes\n",
    "                                    workers=8,            # Número de núcleos de CPU para entrenamiento paralelo\n",
    "                                    sg=1)                 # Algoritmo: 1 para Skip-gram, 0 para CBOW\n",
    "\n",
    "    # Creación del vocabulario a partir del corpus\n",
    "    w2v_tp.build_vocab([e2 for e1 in comp_data[\"title_tokens\"].values for e2 in e1],\n",
    "                    progress_per=10000)\n",
    "\n",
    "    # Entrenamiento del modelo Word2Vec\n",
    "    w2v_tp.train([e2 for e1 in comp_data[\"title_tokens\"].values for e2 in e1],\n",
    "                total_examples=w2v_tp.corpus_count,\n",
    "                epochs=30, report_delay=1)\n",
    "\n",
    "    ## Ejercicio de predicción con word2vec\n",
    "\n",
    "    # Obtención de embeddings de títulos utilizando el modelo Word2Vec\n",
    "    title_embs = comp_data[\"title_tokens\"].map(lambda x: average_vectors(x, w2v_tp, STOP_WORDS_SP))\n",
    "    title_embs= np.array(title_embs.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA \n",
    "sobre los embedding de w2v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if w2v:\n",
    "    # Confirmamos que las matriz de covarianza de las variables estandarizadas es igual a la de correlación de las originales\n",
    "    scaler = StandardScaler(with_std=True, with_mean=True)\n",
    "    title_embs_scaled = pd.DataFrame(scaler.fit_transform(title_embs))\n",
    "    title_embs_scaled.cov()\n",
    "\n",
    "    n_components = 30\n",
    "    # Realiza el análisis de componentes principales (PCA) en los datos escalados\n",
    "    pca_title_embs = PCA(n_components = n_components)\n",
    "    pca_title_embs.fit(title_embs_scaled)\n",
    "\n",
    "    # Crea un DataFrame para almacenar los resultados del PCA\n",
    "    scores = pd.DataFrame(pca_title_embs.transform(title_embs_scaled), index=comp_data.index)\n",
    "    scores.head()\n",
    "\n",
    "    # Veamos qué valores de los loadings se obtuvieron (cada fila es un vector de loadins)\n",
    "    print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une compdata con los embs de los titulos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if w2v:\n",
    "    # title_embs_df = pd.DataFrame(title_embs, columns=[f\"emb_{i}\" for i in range(title_embs.shape[1])])\n",
    "\n",
    "    # # Concatenate comp_data with title_embs_df along the columns axis (axis=1)\n",
    "    # comp_data = pd.concat([comp_data, title_embs_df], axis=1)\n",
    "    comp_data = pd.concat([comp_data, scores], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropeo \n",
    "atributos que no ayudan al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data = comp_data.drop(\n",
    "    columns=['accepts_mercadopago', \n",
    "            # 'available_quantity',\n",
    "            # 'avg_gmv_item_domain_30days',\n",
    "            # 'avg_gmv_item_sel',\n",
    "            # 'avg_gmv_seller_bday',\n",
    "            # 'avg_qty_orders_item_domain_30days',\n",
    "            # 'avg_qty_orders_item_sel_30days',\n",
    "            # 'avg_si_item_sel_30day',\n",
    "            'benefit',              \n",
    "            'boosted',\n",
    "            'category_id',          \n",
    "            # 'conversion',\n",
    "            'date',             #chiche d efechas separar dia mes etc\n",
    "            'deal_print_id',\n",
    "            'domain_id',\n",
    "            'etl_version',\n",
    "            'free_shipping',    \n",
    "            'fulfillment',\n",
    "            'full_name',\n",
    "            # 'health',\n",
    "            # 'is_pdp',\n",
    "            'product_id',\n",
    "            'item_id',\n",
    "            # 'listing_type_id', #hacer OHE\n",
    "            # 'logistic_type',   #hacer OHE\n",
    "            'main_picture',\n",
    "            # 'offset',\n",
    "            # 'original_price',\n",
    "            # 'platform',        #hacer OHE\n",
    "            # 'price',\n",
    "            # 'print_position',\n",
    "            'print_server_timestamp',\n",
    "            # 'qty_items_dom',\n",
    "            # 'qty_items_sel',\n",
    "            'site_id',\n",
    "            # 'sold_quantity',\n",
    "            'tags',             #experimentar\n",
    "            'title',            #W2vec\n",
    "            # 'total_asp_item_domain_30days',\n",
    "            # 'total_asp_item_sel_30days',\n",
    "            # 'total_gmv_domain_bday',\n",
    "            # 'total_gmv_item_30days',\n",
    "            # 'total_items_domain',\n",
    "            # 'total_items_seller',\n",
    "            # 'total_orders_domain_30days',\n",
    "            # 'total_orders_item_30days',\n",
    "            # 'total_orders_sel_30days',\n",
    "            # 'total_si_domain_30days',\n",
    "            # 'total_si_item_30days',\n",
    "            # 'total_si_sel_30days',\n",
    "            # 'total_visits_domain',\n",
    "            # 'total_visits_item',\n",
    "            # 'total_visits_seller',\n",
    "            'uid',\n",
    "            'user_id',\n",
    "            'warranty',\n",
    "            # 'ROW_ID',\n",
    "            # 'title_tokens',\n",
    "            ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "available_quantity                     int64\n",
       "avg_gmv_item_domain_30days           float64\n",
       "avg_gmv_item_sel                     float64\n",
       "avg_gmv_seller_bday                  float64\n",
       "avg_qty_orders_item_domain_30days    float64\n",
       "                                      ...   \n",
       "offset2 + health2                    float64\n",
       "offset2 + original_price2              int64\n",
       "price2 + health2                     float64\n",
       "price2 + original_price2               int64\n",
       "health2 + original_price2            float64\n",
       "Length: 74, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Set - HOLDOUT SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split into training and evaluation samples\n",
    "train_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "del comp_data\n",
    "gc.collect()\n",
    "\n",
    "# train_data = pd.get_dummies(train_data, columns=['is_pdp','listing_type_id','logistic_type','platform','month','dayofweek'])\n",
    "y_train_all = train_data[\"conversion\"]\n",
    "X_train_all = train_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "del train_data\n",
    "gc.collect()\n",
    "\n",
    "#hold out set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.2, random_state=random_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OHE \n",
    "en las categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = pd.get_dummies(X_train, columns=['is_pdp','listing_type_id','logistic_type','platform','month','dayofweek']) \n",
    "\n",
    "X_val = pd.get_dummies(X_val, columns=['is_pdp','listing_type_id','logistic_type','platform','month','dayofweek']) \n",
    "\n",
    "eval_data = pd.get_dummies(eval_data, columns=['is_pdp','listing_type_id','logistic_type','platform','month','dayofweek'])\n",
    "\n",
    "X_train_all = pd.get_dummies(X_train_all, columns=['is_pdp','listing_type_id','logistic_type','platform','month','dayofweek'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# align X_train and eval_data\n",
    "eval_data_rowid = eval_data[\"ROW_ID\"]\n",
    "\n",
    "X_train_all, eval_data = X_train_all.align(eval_data, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Concatenar los DataFrames\n",
    "eval_data = pd.concat([eval_data, eval_data_rowid], axis=1)\n",
    "\n",
    "# Ahora eval_data contiene las columnas de eval_data, eval_data_conversion y eval_data_rowid concatenadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dtype('int64') dtype('float64') dtype('int32') dtype('bool')]\n"
     ]
    }
   ],
   "source": [
    "# X_train_all.dtypes\n",
    "unique_data_types = X_train_all.dtypes.unique()\n",
    "print(unique_data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19211, 92)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de aprendizaje - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperopt para buscar los parametros para xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 12/80 [11:16<49:23, 43.58s/trial, best loss: 0.08910986944014165]  "
     ]
    }
   ],
   "source": [
    "greedy = False\n",
    "\n",
    "if not greedy:    \n",
    "# Define el espacio de búsqueda de hiperparámetros para XGBoost\n",
    "    space = {\n",
    "        'max_depth': hp.quniform('max_depth', 3, 11, 1),\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.01, 0.1),\n",
    "        'gamma': hp.uniform('gamma', 1, 20),\n",
    "        'reg_lambda': hp.uniform('reg_lambda', 0, 15),\n",
    "        'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "        'min_child_weight': hp.uniform('min_child_weight', 0, 100),\n",
    "        'colsample_bytree': hp.uniform('colsample_bytree', 0.65, 1),\n",
    "        'n_estimators': hp.quniform('n_estimators', 500, 2500, 1)\n",
    "    }\n",
    "\n",
    "    # Función objetivo para la optimización\n",
    "    def objective(params):\n",
    "        params['n_estimators'] = int(params['n_estimators'])  # Redondea n_estimators a un valor entero\n",
    "        params['max_depth'] = int(params['max_depth'])  # Redondea max_depth a un valor entero\n",
    "        clf_xgb = xgb.XGBClassifier(objective='binary:logistic', seed=42, eval_metric='auc', tree_method='gpu_hist', **params)\n",
    "        cv_scores = cross_val_score(clf_xgb, X_train, y_train, cv=KFold(4))\n",
    "        mean_auc_roc = np.mean(cv_scores)\n",
    "        return {'loss': 1 - mean_auc_roc, 'status': STATUS_OK}\n",
    "\n",
    "    # Realiza la optimización con Hyperopt\n",
    "    trials = Trials()\n",
    "    best = fmin(objective, space, algo=tpe.suggest, max_evals=80, rstate=np.random.default_rng(22))\n",
    "\n",
    "    # Obtiene los mejores hiperparámetros encontrados\n",
    "    best_grid = space_eval(space, best)\n",
    "\n",
    "    print(\"Mejores hiperparámetros encontrados para XGBoost:\")\n",
    "    print(best_grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not greedy:    \n",
    "# Define el espacio de búsqueda de hiperparámetros para XGBoost\n",
    "    space = {\n",
    "        'max_depth': hp.quniform('max_depth', 3, 11, 1),\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.01, 0.1),\n",
    "        'gamma': hp.uniform('gamma', 1, 20),\n",
    "        'reg_lambda': hp.uniform('reg_lambda', 0, 15),\n",
    "        'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "        'min_child_weight': hp.uniform('min_child_weight', 0, 100),\n",
    "        'colsample_bytree': hp.uniform('colsample_bytree', 0.65, 1),\n",
    "        'n_estimators': hp.quniform('n_estimators', 500, 2500, 1)\n",
    "    }\n",
    "\n",
    "    # Función objetivo para la optimización\n",
    "    def objective(params):\n",
    "        params['n_estimators'] = int(params['n_estimators'])  # Redondea n_estimators a un valor entero\n",
    "        params['max_depth'] = int(params['max_depth'])  # Redondea max_depth a un valor entero\n",
    "        clf_xgb = xgb.XGBClassifier(objective='binary:logistic', seed=42, eval_metric='auc', tree_method='gpu_hist', **params)\n",
    "        cv_scores = cross_val_score(clf_xgb, X_train, y_train, cv=KFold(4))\n",
    "        mean_auc_roc = np.mean(cv_scores)\n",
    "        return {'loss': 1 - mean_auc_roc, 'status': STATUS_OK}\n",
    "\n",
    "    # Realiza la optimización con Hyperopt\n",
    "    trials = Trials()\n",
    "    best = fmin(objective, space, algo=tpe.suggest, max_evals=80, rstate=np.random.default_rng(22))\n",
    "\n",
    "    # Obtiene los mejores hiperparámetros encontrados\n",
    "    best_grid = space_eval(space, best)\n",
    "\n",
    "    print(\"Mejores hiperparámetros encontrados para XGBoost:\")\n",
    "    print(best_grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "greedy serch apra buscar parametros de xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc-AUC actual; 0.8851612430252037\n",
      "Con grid: {'colsample_bytree': 0.7810890415965769, 'gamma': 9.50714306409916, 'learning_rate': 0.03659969709057025, 'max_depth': 7, 'min_child_weight': 9.361118426546192, 'n_estimators': 1166, 'reg_lambda': 1.4996237372700434, 'subsample': 0.7296244459829335}\n",
      "----------\n",
      "Mejor valor de ROC-AUC encontrado: 0.8851612430252037\n",
      "Grid{'colsample_bytree': 0.7810890415965769, 'gamma': 9.50714306409916, 'learning_rate': 0.03659969709057025, 'max_depth': 7, 'min_child_weight': 9.361118426546192, 'n_estimators': 1166, 'reg_lambda': 1.4996237372700434, 'subsample': 0.7296244459829335}\n",
      "----------\n",
      "Roc-AUC actual; 0.8918087478020738\n",
      "Con grid: {'colsample_bytree': 0.7667980138986576, 'gamma': 1.4286681792194078, 'learning_rate': 0.032544423647442644, 'max_depth': 7, 'min_child_weight': 58.19459112971966, 'n_estimators': 2215, 'reg_lambda': 14.078290635236252, 'subsample': 0.5003893829205072}\n",
      "----------\n",
      "Mejor valor de ROC-AUC encontrado: 0.8918087478020738\n",
      "Grid{'colsample_bytree': 0.7667980138986576, 'gamma': 1.4286681792194078, 'learning_rate': 0.032544423647442644, 'max_depth': 7, 'min_child_weight': 58.19459112971966, 'n_estimators': 2215, 'reg_lambda': 14.078290635236252, 'subsample': 0.5003893829205072}\n",
      "----------\n",
      "Roc-AUC actual; 0.8905943735829438\n",
      "Con grid: {'colsample_bytree': 0.9972740457519261, 'gamma': 6.1748150962771655, 'learning_rate': 0.030582658024414046, 'max_depth': 11, 'min_child_weight': 17.473748411882514, 'n_estimators': 1782, 'reg_lambda': 5.997914575728832, 'subsample': 0.5233328316068078}\n",
      "Roc-AUC actual; 0.8832729279835285\n",
      "Con grid: {'colsample_bytree': 0.9908144315945107, 'gamma': 2.3277134043030423, 'learning_rate': 0.00453032172664104, 'max_depth': 5, 'min_child_weight': 22.947719476029764, 'n_estimators': 1967, 'reg_lambda': 8.886218532930638, 'subsample': 0.5232252063599989}\n",
      "Roc-AUC actual; 0.8738497895361187\n",
      "Con grid: {'colsample_bytree': 0.8626406981655035, 'gamma': 1.7052412368729153, 'learning_rate': 0.003252579649263976, 'max_depth': 6, 'min_child_weight': 56.53210534109117, 'n_estimators': 713, 'reg_lambda': 12.125960221746917, 'subsample': 0.6523068845866853}\n",
      "Roc-AUC actual; 0.8861317874092205\n",
      "Con grid: {'colsample_bytree': 0.6841852399022343, 'gamma': 6.842330265121569, 'learning_rate': 0.022007624686980067, 'max_depth': 9, 'min_child_weight': 36.59979946695726, 'n_estimators': 1475, 'reg_lambda': 0.5158278167282759, 'subsample': 0.954660201039391}\n",
      "Roc-AUC actual; 0.8860122666450309\n",
      "Con grid: {'colsample_bytree': 0.7405729935600059, 'gamma': 6.62522284353982, 'learning_rate': 0.015585553804470548, 'max_depth': 8, 'min_child_weight': 12.47649977209133, 'n_estimators': 2495, 'reg_lambda': 2.7728168328829055, 'subsample': 0.9847923138822793}\n",
      "Roc-AUC actual; 0.8855520062355503\n",
      "Con grid: {'colsample_bytree': 0.9212964881763901, 'gamma': 9.394989415641891, 'learning_rate': 0.04474136752138244, 'max_depth': 10, 'min_child_weight': 34.22663846432396, 'n_estimators': 1975, 'reg_lambda': 0.678409333658071, 'subsample': 0.6626651653816322}\n",
      "Roc-AUC actual; 0.8927635254846412\n",
      "Con grid: {'colsample_bytree': 0.7860370513913187, 'gamma': 2.713490317738959, 'learning_rate': 0.04143687545759647, 'max_depth': 11, 'min_child_weight': 16.856070581242847, 'n_estimators': 1079, 'reg_lambda': 4.1399877303381505, 'subsample': 0.6481367528520412}\n",
      "----------\n",
      "Mejor valor de ROC-AUC encontrado: 0.8927635254846412\n",
      "Grid{'colsample_bytree': 0.7860370513913187, 'gamma': 2.713490317738959, 'learning_rate': 0.04143687545759647, 'max_depth': 11, 'min_child_weight': 16.856070581242847, 'n_estimators': 1079, 'reg_lambda': 4.1399877303381505, 'subsample': 0.6481367528520412}\n",
      "----------\n",
      "Roc-AUC actual; 0.8877788180725221\n",
      "Con grid: {'colsample_bytree': 0.7078434286720509, 'gamma': 0.1563640674119393, 'learning_rate': 0.02117007403531848, 'max_depth': 3, 'min_child_weight': 11.922940892050345, 'n_estimators': 2195, 'reg_lambda': 0.21119734072626684, 'subsample': 0.5994212020444025}\n",
      "Roc-AUC actual; 0.8860146874989931\n",
      "Con grid: {'colsample_bytree': 0.8989696834620275, 'gamma': 7.9017554053120564, 'learning_rate': 0.03029799873905057, 'max_depth': 9, 'min_child_weight': 39.06462153011667, 'n_estimators': 1751, 'reg_lambda': 12.946551388133903, 'subsample': 0.811649063413779}\n",
      "Roc-AUC actual; 0.8928493233971283\n",
      "Con grid: {'colsample_bytree': 0.7658143086984273, 'gamma': 0.6355835028602363, 'learning_rate': 0.015549116085783111, 'max_depth': 10, 'min_child_weight': 39.9553413970498, 'n_estimators': 1383, 'reg_lambda': 9.563362070328196, 'subsample': 0.9436063712881633}\n",
      "----------\n",
      "Mejor valor de ROC-AUC encontrado: 0.8928493233971283\n",
      "Grid{'colsample_bytree': 0.7658143086984273, 'gamma': 0.6355835028602363, 'learning_rate': 0.015549116085783111, 'max_depth': 10, 'min_child_weight': 39.9553413970498, 'n_estimators': 1383, 'reg_lambda': 9.563362070328196, 'subsample': 0.9436063712881633}\n",
      "----------\n",
      "Roc-AUC actual; 0.8865664419479308\n",
      "Con grid: {'colsample_bytree': 0.8152752238066823, 'gamma': 1.195942459383017, 'learning_rate': 0.035662239361149754, 'max_depth': 3, 'min_child_weight': 43.303771269892394, 'n_estimators': 1202, 'reg_lambda': 11.564507699318415, 'subsample': 0.7468977981821954}\n",
      "Roc-AUC actual; 0.8709092040387036\n",
      "Con grid: {'colsample_bytree': 0.8329564902836979, 'gamma': 4.275410183585496, 'learning_rate': 0.0012709563372047595, 'max_depth': 5, 'min_child_weight': 1.885751141204055, 'n_estimators': 1442, 'reg_lambda': 7.130553347731676, 'subsample': 0.7816377859881918}\n",
      "Roc-AUC actual; 0.8909888971271103\n",
      "Con grid: {'colsample_bytree': 0.8934306302491446, 'gamma': 1.3933145440587569, 'learning_rate': 0.030220868963890864, 'max_depth': 6, 'min_child_weight': 56.57121423347886, 'n_estimators': 2422, 'reg_lambda': 10.421773995595569, 'subsample': 0.9402339195076288}\n",
      "Roc-AUC actual; 0.8860635495881585\n",
      "Con grid: {'colsample_bytree': 0.8685239168468276, 'gamma': 2.95633685837714, 'learning_rate': 0.005274712991513531, 'max_depth': 6, 'min_child_weight': 53.553539909398665, 'n_estimators': 2054, 'reg_lambda': 4.865175315079109, 'subsample': 0.5610439773503366}\n",
      "Roc-AUC actual; 0.8849987966041757\n",
      "Con grid: {'colsample_bytree': 0.7747042433269412, 'gamma': 9.06828441545754, 'learning_rate': 0.013606612469231767, 'max_depth': 11, 'min_child_weight': 49.080885955349586, 'n_estimators': 2468, 'reg_lambda': 4.571718872370435, 'subsample': 0.5823279265714709}\n",
      "Roc-AUC actual; 0.889672976094441\n",
      "Con grid: {'colsample_bytree': 0.8369312967814047, 'gamma': 4.848299713589832, 'learning_rate': 0.03462180164451352, 'max_depth': 10, 'min_child_weight': 56.57458223475115, 'n_estimators': 2097, 'reg_lambda': 3.2814632935960537, 'subsample': 0.7790510010086706}\n",
      "Roc-AUC actual; 0.8919935515214596\n",
      "Con grid: {'colsample_bytree': 0.7913426598703143, 'gamma': 0.6489224710898156, 'learning_rate': 0.012695770696717235, 'max_depth': 9, 'min_child_weight': 29.834910353543126, 'n_estimators': 1836, 'reg_lambda': 2.2213039493009985, 'subsample': 0.9988702425244709}\n",
      "Roc-AUC actual; 0.8845451757426944\n",
      "Con grid: {'colsample_bytree': 0.7433733549963497, 'gamma': 9.766149558326529, 'learning_rate': 0.020551850665911565, 'max_depth': 9, 'min_child_weight': 16.718787854196687, 'n_estimators': 1948, 'reg_lambda': 9.515270170520457, 'subsample': 0.8403527257773834}\n",
      "Roc-AUC actual; 0.8902550179185024\n",
      "Con grid: {'colsample_bytree': 0.8358271041609977, 'gamma': 4.477831645730916, 'learning_rate': 0.027644654453566397, 'max_depth': 9, 'min_child_weight': 40.328132844352716, 'n_estimators': 902, 'reg_lambda': 5.544816840921067, 'subsample': 0.621079969138713}\n",
      "Roc-AUC actual; 0.8911473028580692\n",
      "Con grid: {'colsample_bytree': 0.9310989147329636, 'gamma': 4.703006344460384, 'learning_rate': 0.04917115704474215, 'max_depth': 8, 'min_child_weight': 32.14648104448551, 'n_estimators': 1451, 'reg_lambda': 11.975176874768266, 'subsample': 0.5753587719827147}\n",
      "Roc-AUC actual; 0.887002547218979\n",
      "Con grid: {'colsample_bytree': 0.8278695718592516, 'gamma': 6.958128067908818, 'learning_rate': 0.042917940240685994, 'max_depth': 6, 'min_child_weight': 0.9952697356713691, 'n_estimators': 2234, 'reg_lambda': 3.3974366279690695, 'subsample': 0.822586395204725}\n",
      "Roc-AUC actual; 0.884398420371429\n",
      "Con grid: {'colsample_bytree': 0.711028250151747, 'gamma': 6.90937738102466, 'learning_rate': 0.01933676731502687, 'max_depth': 4, 'min_child_weight': 50.262606354439676, 'n_estimators': 1172, 'reg_lambda': 3.1360743110657054, 'subsample': 0.770723986913783}\n",
      "Roc-AUC actual; 0.892347556912452\n",
      "Con grid: {'colsample_bytree': 0.8935245397707787, 'gamma': 2.285500217972997, 'learning_rate': 0.008747746354796809, 'max_depth': 11, 'min_child_weight': 30.998153476260857, 'n_estimators': 1580, 'reg_lambda': 3.627784363506775, 'subsample': 0.5465513839029497}\n",
      "Roc-AUC actual; 0.8846624492067903\n",
      "Con grid: {'colsample_bytree': 0.9640255152836643, 'gamma': 9.004180571633304, 'learning_rate': 0.0316550728636634, 'max_depth': 11, 'min_child_weight': 16.732281155530913, 'n_estimators': 2382, 'reg_lambda': 10.889335183053591, 'subsample': 0.9485551299762885}\n",
      "Roc-AUC actual; 0.8832701199709363\n",
      "Con grid: {'colsample_bytree': 0.960480248492791, 'gamma': 7.798755458576238, 'learning_rate': 0.03210158230771439, 'max_depth': 3, 'min_child_weight': 9.697722845676825, 'n_estimators': 2145, 'reg_lambda': 10.034823820713429, 'subsample': 0.7903433107182274}\n",
      "Roc-AUC actual; 0.8834209160322709\n",
      "Con grid: {'colsample_bytree': 0.7802989682966102, 'gamma': 9.401334424577785, 'learning_rate': 0.04868319183776587, 'max_depth': 4, 'min_child_weight': 9.64848308504992, 'n_estimators': 1037, 'reg_lambda': 7.284206303793399, 'subsample': 0.7242120714931237}\n",
      "Roc-AUC actual; 0.8754244393360076\n",
      "Con grid: {'colsample_bytree': 0.9980601119137873, 'gamma': 1.7592525267734538, 'learning_rate': 0.0009037681807760434, 'max_depth': 7, 'min_child_weight': 10.729362553279728, 'n_estimators': 2227, 'reg_lambda': 9.74449348570822, 'subsample': 0.924611705247089}\n",
      "Roc-AUC actual; 0.8814429982432967\n",
      "Con grid: {'colsample_bytree': 0.8801645123051202, 'gamma': 5.683086033354716, 'learning_rate': 0.004683738391404624, 'max_depth': 5, 'min_child_weight': 38.179957091153724, 'n_estimators': 1745, 'reg_lambda': 3.659844650686254, 'subsample': 0.9865052773762228}\n",
      "Roc-AUC actual; 0.8835053566645034\n",
      "Con grid: {'colsample_bytree': 0.7875842036333661, 'gamma': 8.920465551771134, 'learning_rate': 0.031556931299863145, 'max_depth': 4, 'min_child_weight': 21.00470446168054, 'n_estimators': 2332, 'reg_lambda': 8.653558269395386, 'subsample': 0.746258846909432}\n",
      "Roc-AUC actual; 0.8828199568365298\n",
      "Con grid: {'colsample_bytree': 0.7183350457293156, 'gamma': 7.224521152615053, 'learning_rate': 0.01403861812204279, 'max_depth': 5, 'min_child_weight': 46.11324085837854, 'n_estimators': 2235, 'reg_lambda': 2.6566601911057344, 'subsample': 0.9702292921764571}\n",
      "Roc-AUC actual; 0.883730634036071\n",
      "Con grid: {'colsample_bytree': 0.9838750019509056, 'gamma': 9.148643902204485, 'learning_rate': 0.01850793501277222, 'max_depth': 11, 'min_child_weight': 57.184310821437194, 'n_estimators': 2072, 'reg_lambda': 6.422762224759715, 'subsample': 0.9833274095218347}\n",
      "Roc-AUC actual; 0.8860055247447498\n",
      "Con grid: {'colsample_bytree': 0.9872669919812385, 'gamma': 8.530094554673601, 'learning_rate': 0.014722444603479285, 'max_depth': 7, 'min_child_weight': 11.454661869020761, 'n_estimators': 1884, 'reg_lambda': 4.753830077344165, 'subsample': 0.5847463733430462}\n",
      "Roc-AUC actual; 0.883892346190732\n",
      "Con grid: {'colsample_bytree': 0.8448804418604225, 'gamma': 9.36154774160781, 'learning_rate': 0.03480148983374865, 'max_depth': 10, 'min_child_weight': 4.224967850972634, 'n_estimators': 1325, 'reg_lambda': 9.225108400487546, 'subsample': 0.9950269250521316}\n",
      "Roc-AUC actual; 0.88647563322375\n",
      "Con grid: {'colsample_bytree': 0.6990294053327835, 'gamma': 5.183296523637368, 'learning_rate': 0.04386865359639777, 'max_depth': 3, 'min_child_weight': 50.920187695480465, 'n_estimators': 2116, 'reg_lambda': 10.53726125980664, 'subsample': 0.6797455756098776}\n",
      "Roc-AUC actual; 0.8860506932515829\n",
      "Con grid: {'colsample_bytree': 0.7527571454925727, 'gamma': 8.093611554785136, 'learning_rate': 0.04050566973395904, 'max_depth': 11, 'min_child_weight': 36.690822651940834, 'n_estimators': 1527, 'reg_lambda': 7.670135982914067, 'subsample': 0.7507581473435998}\n",
      "Roc-AUC actual; 0.8871801284273707\n",
      "Con grid: {'colsample_bytree': 0.9294033126383713, 'gamma': 6.4996393077776515, 'learning_rate': 0.03509834386288517, 'max_depth': 5, 'min_child_weight': 20.27970941109215, 'n_estimators': 1815, 'reg_lambda': 8.68297343261338, 'subsample': 0.7192370615090435}\n",
      "Roc-AUC actual; 0.8911007058693943\n",
      "Con grid: {'colsample_bytree': 0.8852091473533198, 'gamma': 3.2815266747473193, 'learning_rate': 0.00775208083638721, 'max_depth': 8, 'min_child_weight': 35.44999563414065, 'n_estimators': 2065, 'reg_lambda': 3.753770407737961, 'subsample': 0.5194173672147115}\n",
      "Roc-AUC actual; 0.888642088365335\n",
      "Con grid: {'colsample_bytree': 0.7561429301356279, 'gamma': 5.370824271966555, 'learning_rate': 0.016332562089802046, 'max_depth': 10, 'min_child_weight': 46.19961318591665, 'n_estimators': 1703, 'reg_lambda': 14.478777455861547, 'subsample': 0.7286325808068643}\n",
      "Roc-AUC actual; 0.889857147899741\n",
      "Con grid: {'colsample_bytree': 0.9447080762541935, 'gamma': 1.9438003399487302, 'learning_rate': 0.02056769525283393, 'max_depth': 5, 'min_child_weight': 38.24579408989239, 'n_estimators': 1355, 'reg_lambda': 14.63778119193802, 'subsample': 0.7581501741505976}\n",
      "Roc-AUC actual; 0.8852011381644903\n",
      "Con grid: {'colsample_bytree': 0.763034765529436, 'gamma': 7.9518619476870365, 'learning_rate': 0.013541612563103711, 'max_depth': 9, 'min_child_weight': 15.051631640799672, 'n_estimators': 2197, 'reg_lambda': 14.439726220168875, 'subsample': 0.9179900602561029}\n",
      "Roc-AUC actual; 0.881047606929787\n",
      "Con grid: {'colsample_bytree': 0.8935909721327943, 'gamma': 4.089529444142698, 'learning_rate': 0.008664716003542289, 'max_depth': 3, 'min_child_weight': 26.541133783866265, 'n_estimators': 1819, 'reg_lambda': 8.238399970591807, 'subsample': 0.8572979613500311}\n",
      "Roc-AUC actual; 0.8920847874551633\n",
      "Con grid: {'colsample_bytree': 0.8810690818512059, 'gamma': 2.7993389694594284, 'learning_rate': 0.047743264033159705, 'max_depth': 9, 'min_child_weight': 39.42667971001001, 'n_estimators': 1280, 'reg_lambda': 9.175811193515283, 'subsample': 0.709800031213895}\n",
      "Roc-AUC actual; 0.892438797296255\n",
      "Con grid: {'colsample_bytree': 0.7367058463254051, 'gamma': 3.559726786512616, 'learning_rate': 0.037892305523218456, 'max_depth': 11, 'min_child_weight': 20.512780002984094, 'n_estimators': 1943, 'reg_lambda': 0.6900396303262912, 'subsample': 0.520364401159485}\n",
      "Roc-AUC actual; 0.8865625748117302\n",
      "Con grid: {'colsample_bytree': 0.9494112044038525, 'gamma': 7.036578593800237, 'learning_rate': 0.02370869145436626, 'max_depth': 7, 'min_child_weight': 28.40830624683394, 'n_estimators': 2012, 'reg_lambda': 6.9701071940941715, 'subsample': 0.8248868413213817}\n",
      "Roc-AUC actual; 0.8837091756577509\n",
      "Con grid: {'colsample_bytree': 0.6668206234689619, 'gamma': 9.491457315913859, 'learning_rate': 0.04433401936490238, 'max_depth': 6, 'min_child_weight': 22.476756877588272, 'n_estimators': 1461, 'reg_lambda': 14.001544621192245, 'subsample': 0.7505199419576296}\n",
      "Roc-AUC actual; 0.8855219547156837\n",
      "Con grid: {'colsample_bytree': 0.8387821069400219, 'gamma': 6.839637693981411, 'learning_rate': 0.030792558219495694, 'max_depth': 9, 'min_child_weight': 56.655095827510046, 'n_estimators': 1595, 'reg_lambda': 9.54605396169017, 'subsample': 0.9004746473411999}\n",
      "Roc-AUC actual; 0.8831671535757535\n",
      "Con grid: {'colsample_bytree': 0.8870089198340436, 'gamma': 5.733670416719333, 'learning_rate': 0.006425017661695509, 'max_depth': 5, 'min_child_weight': 49.23836854463732, 'n_estimators': 1991, 'reg_lambda': 5.791539567011614, 'subsample': 0.9805952819119571}\n",
      "Roc-AUC actual; 0.8750545559910858\n",
      "Con grid: {'colsample_bytree': 0.9668727246846223, 'gamma': 1.9579113478929644, 'learning_rate': 0.003468065043758273, 'max_depth': 4, 'min_child_weight': 2.3377879386140177, 'n_estimators': 1453, 'reg_lambda': 10.245101601245352, 'subsample': 0.5355943242301144}\n",
      "Roc-AUC actual; 0.8694773000707049\n",
      "Con grid: {'colsample_bytree': 0.7616414706028165, 'gamma': 8.448753109694545, 'learning_rate': 0.0011635967867912934, 'max_depth': 4, 'min_child_weight': 15.641671694154187, 'n_estimators': 1919, 'reg_lambda': 10.393355380379091, 'subsample': 0.6741683022265987}\n",
      "Roc-AUC actual; 0.8932312576106708\n",
      "Con grid: {'colsample_bytree': 0.9778268531494976, 'gamma': 0.39186326675646255, 'learning_rate': 0.02089730158577894, 'max_depth': 11, 'min_child_weight': 32.87831299488524, 'n_estimators': 1156, 'reg_lambda': 8.527804256249375, 'subsample': 0.7879622876290423}\n",
      "----------\n",
      "Mejor valor de ROC-AUC encontrado: 0.8932312576106708\n",
      "Grid{'colsample_bytree': 0.9778268531494976, 'gamma': 0.39186326675646255, 'learning_rate': 0.02089730158577894, 'max_depth': 11, 'min_child_weight': 32.87831299488524, 'n_estimators': 1156, 'reg_lambda': 8.527804256249375, 'subsample': 0.7879622876290423}\n",
      "----------\n",
      "Roc-AUC actual; 0.8927059189505571\n",
      "Con grid: {'colsample_bytree': 0.906076714354874, 'gamma': 1.276897294224656, 'learning_rate': 0.012500822460805234, 'max_depth': 10, 'min_child_weight': 52.026996432583324, 'n_estimators': 2459, 'reg_lambda': 12.876191277645177, 'subsample': 0.7144970136875092}\n",
      "Roc-AUC actual; 0.8733412945014593\n",
      "Con grid: {'colsample_bytree': 0.912804873727024, 'gamma': 7.545428740846823, 'learning_rate': 0.00515619344179663, 'max_depth': 3, 'min_child_weight': 32.059648411833805, 'n_estimators': 1295, 'reg_lambda': 12.396861991616124, 'subsample': 0.6600248005153059}\n",
      "Roc-AUC actual; 0.8746421964493442\n",
      "Con grid: {'colsample_bytree': 0.9634331299736701, 'gamma': 3.892016787341631, 'learning_rate': 0.000541882574014918, 'max_depth': 11, 'min_child_weight': 50.82858864573539, 'n_estimators': 964, 'reg_lambda': 4.789704563856223, 'subsample': 0.9750309835254025}\n",
      "Roc-AUC actual; 0.8883651987432991\n",
      "Con grid: {'colsample_bytree': 0.9827125014281446, 'gamma': 5.734378881232861, 'learning_rate': 0.031591860608489966, 'max_depth': 7, 'min_child_weight': 5.5022488355088655, 'n_estimators': 1095, 'reg_lambda': 4.929968180548739, 'subsample': 0.8362592280385193}\n",
      "Roc-AUC actual; 0.8861748732696301\n",
      "Con grid: {'colsample_bytree': 0.913331085303188, 'gamma': 7.915790437258485, 'learning_rate': 0.0394809071397277, 'max_depth': 7, 'min_child_weight': 29.66521828215489, 'n_estimators': 1634, 'reg_lambda': 14.938968630454937, 'subsample': 0.527935577338675}\n",
      "Roc-AUC actual; 0.8871764303949393\n",
      "Con grid: {'colsample_bytree': 0.9079624448768531, 'gamma': 5.45915770164989, 'learning_rate': 0.03529157419393473, 'max_depth': 11, 'min_child_weight': 45.690637903048334, 'n_estimators': 1191, 'reg_lambda': 13.00303057966262, 'subsample': 0.9192403818819925}\n",
      "Roc-AUC actual; 0.8900007526007758\n",
      "Con grid: {'colsample_bytree': 0.7991320162664294, 'gamma': 2.2257641757103053, 'learning_rate': 0.019832580097712594, 'max_depth': 4, 'min_child_weight': 4.880926838513986, 'n_estimators': 1888, 'reg_lambda': 3.4984210863679173, 'subsample': 0.7906527086133912}\n",
      "Roc-AUC actual; 0.884481281218447\n",
      "Con grid: {'colsample_bytree': 0.9520984826961514, 'gamma': 8.803599686384167, 'learning_rate': 0.011834259684103938, 'max_depth': 9, 'min_child_weight': 59.160063829372255, 'n_estimators': 1855, 'reg_lambda': 5.253273700504703, 'subsample': 0.8540905977915999}\n",
      "Roc-AUC actual; 0.8916786180014025\n",
      "Con grid: {'colsample_bytree': 0.8185834457874404, 'gamma': 3.7798759012225447, 'learning_rate': 0.035254218214832996, 'max_depth': 5, 'min_child_weight': 6.671848938369081, 'n_estimators': 2174, 'reg_lambda': 3.8052326080970813, 'subsample': 0.7026010433907086}\n",
      "Roc-AUC actual; 0.8852781471310663\n",
      "Con grid: {'colsample_bytree': 0.8500155588217537, 'gamma': 7.409738609106683, 'learning_rate': 0.03835974265542859, 'max_depth': 11, 'min_child_weight': 44.652699416597876, 'n_estimators': 1086, 'reg_lambda': 8.750531476457395, 'subsample': 0.981086274237271}\n",
      "Roc-AUC actual; 0.8889012309917795\n",
      "Con grid: {'colsample_bytree': 0.7812047028332965, 'gamma': 2.8571208628186073, 'learning_rate': 0.04342995640947302, 'max_depth': 8, 'min_child_weight': 26.18014989639336, 'n_estimators': 757, 'reg_lambda': 0.18231712034724512, 'subsample': 0.9849394133538195}\n",
      "Roc-AUC actual; 0.8846062711545478\n",
      "Con grid: {'colsample_bytree': 0.6651059691827017, 'gamma': 8.911431136980712, 'learning_rate': 0.026385055454314993, 'max_depth': 7, 'min_child_weight': 15.179410079354467, 'n_estimators': 2382, 'reg_lambda': 14.539538034286485, 'subsample': 0.7615489220850744}\n",
      "Roc-AUC actual; 0.886770203089889\n",
      "Con grid: {'colsample_bytree': 0.8702895233473419, 'gamma': 6.957486889846171, 'learning_rate': 0.022727053238388664, 'max_depth': 7, 'min_child_weight': 41.81550335182889, 'n_estimators': 1982, 'reg_lambda': 13.517370157364835, 'subsample': 0.522723190170729}\n",
      "Roc-AUC actual; 0.88477372838727\n",
      "Con grid: {'colsample_bytree': 0.7483371163572806, 'gamma': 9.504114840765588, 'learning_rate': 0.04451318919454582, 'max_depth': 10, 'min_child_weight': 37.207955868092206, 'n_estimators': 1731, 'reg_lambda': 2.545198558433483, 'subsample': 0.8234241808343619}\n",
      "Roc-AUC actual; 0.8917009174484705\n",
      "Con grid: {'colsample_bytree': 0.7858939343964553, 'gamma': 2.2939474570494967, 'learning_rate': 0.013296896248211948, 'max_depth': 7, 'min_child_weight': 58.46368845996999, 'n_estimators': 2427, 'reg_lambda': 6.798612702432465, 'subsample': 0.5161579755548252}\n",
      "Roc-AUC actual; 0.8889898057661645\n",
      "Con grid: {'colsample_bytree': 0.747917227509131, 'gamma': 4.112067208721863, 'learning_rate': 0.03013909410116493, 'max_depth': 5, 'min_child_weight': 49.35223457539014, 'n_estimators': 2317, 'reg_lambda': 6.249585866077188, 'subsample': 0.7905813105904131}\n",
      "Roc-AUC actual; 0.8923595232291884\n",
      "Con grid: {'colsample_bytree': 0.9717117931424459, 'gamma': 0.8274842003530614, 'learning_rate': 0.04383307429628964, 'max_depth': 7, 'min_child_weight': 1.5820184698351203, 'n_estimators': 1073, 'reg_lambda': 12.158299961727494, 'subsample': 0.9936380646574723}\n",
      "Roc-AUC actual; 0.8872128055057623\n",
      "Con grid: {'colsample_bytree': 0.7026459118862349, 'gamma': 5.941307153521351, 'learning_rate': 0.019044542831551076, 'max_depth': 8, 'min_child_weight': 49.74985324239086, 'n_estimators': 1388, 'reg_lambda': 12.574930570667068, 'subsample': 0.7343465798974851}\n",
      "Roc-AUC actual; 0.8807788253884852\n",
      "Con grid: {'colsample_bytree': 0.7951868258181828, 'gamma': 2.7340707193070624, 'learning_rate': 0.002818774832546356, 'max_depth': 8, 'min_child_weight': 41.50286383013472, 'n_estimators': 1408, 'reg_lambda': 14.995765099291958, 'subsample': 0.9983184185369527}\n",
      "Roc-AUC actual; 0.8867272685328519\n",
      "Con grid: {'colsample_bytree': 0.8444010969609196, 'gamma': 7.689874151805105, 'learning_rate': 0.0472382864941214, 'max_depth': 5, 'min_child_weight': 14.84088610459186, 'n_estimators': 1801, 'reg_lambda': 3.0756799275978737, 'subsample': 0.6257212197558992}\n",
      "Roc-AUC actual; 0.8905533703689575\n",
      "Con grid: {'colsample_bytree': 0.7461561289590681, 'gamma': 2.0722764900805646, 'learning_rate': 0.043911033373959615, 'max_depth': 10, 'min_child_weight': 37.08769442747375, 'n_estimators': 2071, 'reg_lambda': 4.0300872303796105, 'subsample': 0.5110923710151508}\n",
      "Roc-AUC actual; 0.8889772119854414\n",
      "Con grid: {'colsample_bytree': 0.8243578153260626, 'gamma': 4.7621069678901415, 'learning_rate': 0.041568574503125066, 'max_depth': 4, 'min_child_weight': 51.13089001911241, 'n_estimators': 1777, 'reg_lambda': 14.519606481597346, 'subsample': 0.5442041116828233}\n",
      "Roc-AUC actual; 0.8871244576864366\n",
      "Con grid: {'colsample_bytree': 0.9271362438143715, 'gamma': 5.8995589713763605, 'learning_rate': 0.024002298350923757, 'max_depth': 9, 'min_child_weight': 45.30823534041714, 'n_estimators': 1552, 'reg_lambda': 9.590420354613634, 'subsample': 0.9025223337596809}\n",
      "Roc-AUC actual; 0.8874826906716646\n",
      "Con grid: {'colsample_bytree': 0.9661028705368192, 'gamma': 6.1726371209729205, 'learning_rate': 0.0490231362546189, 'max_depth': 4, 'min_child_weight': 35.395061211277984, 'n_estimators': 1497, 'reg_lambda': 8.322234128133385, 'subsample': 0.5455010455256651}\n",
      "Roc-AUC actual; 0.8884461949987541\n",
      "Con grid: {'colsample_bytree': 0.9042389614275799, 'gamma': 5.474463068991115, 'learning_rate': 0.02254552237080522, 'max_depth': 9, 'min_child_weight': 31.41613656090067, 'n_estimators': 780, 'reg_lambda': 14.242809354864631, 'subsample': 0.573536740464519}\n",
      "Roc-AUC actual; 0.8918158679607864\n",
      "Con grid: {'colsample_bytree': 0.974305668806523, 'gamma': 4.921162930795382, 'learning_rate': 0.012912219414947919, 'max_depth': 10, 'min_child_weight': 2.751675984277977, 'n_estimators': 2497, 'reg_lambda': 7.389271409893044, 'subsample': 0.664375805143754}\n",
      "Roc-AUC actual; 0.8890843970746614\n",
      "Con grid: {'colsample_bytree': 0.871690299010854, 'gamma': 2.401456187781931, 'learning_rate': 0.0037931664054331963, 'max_depth': 11, 'min_child_weight': 27.607149539014323, 'n_estimators': 1547, 'reg_lambda': 2.278540402684415, 'subsample': 0.5694135863247051}\n",
      "Roc-AUC actual; 0.8885762001966482\n",
      "Con grid: {'colsample_bytree': 0.8743061606811251, 'gamma': 1.8188008439914483, 'learning_rate': 0.017283364166193162, 'max_depth': 5, 'min_child_weight': 52.45905629126503, 'n_estimators': 1366, 'reg_lambda': 10.013366077815407, 'subsample': 0.5861599356008149}\n"
     ]
    }
   ],
   "source": [
    "if greedy:\n",
    "        \n",
    "    #ver uan forma de tener mejor parametos que gerar\n",
    "    params = {'max_depth': list(range(3, 12)),\n",
    "            'learning_rate': uniform(scale = 0.05),\n",
    "            'gamma': uniform(scale=10),               #chiche va de 1 a 20\n",
    "            'reg_lambda': uniform(scale = 15),        # Parámetro de regularización.\n",
    "            'subsample': uniform(0.5, 0.5),          # Entre 0.5 y 1.\n",
    "            'min_child_weight': uniform(scale = 60),   #0 a 100\n",
    "            'colsample_bytree': uniform(0.65, 0.35), # Entre 0.75 y 1.\n",
    "            'n_estimators': list(range(700, 2500))    #ma que 500 tambein a la noche para que corra\n",
    "            }\n",
    "\n",
    "    l_auc = []\n",
    "    l_grid = []\n",
    "    start = time.time()\n",
    "    random_state = 42\n",
    "    best_score = 0\n",
    "    best_estimator = None\n",
    "    iterations = 80\n",
    "    i=1\n",
    "    for g in ParameterSampler(params, n_iter = iterations, random_state = random_state):\n",
    "        clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic', seed = random_state, eval_metric = 'auc', **g, tree_method = 'gpu_hist') #enable_categorical = True\n",
    "        clf_xgb.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False)\n",
    "\n",
    "        y_pred = clf_xgb.predict_proba(X_val)[:, 1] # Obtenemos la probabilidad de una de las clases (cualquiera).\n",
    "        auc_roc = sklearn.metrics.roc_auc_score(y_val, y_pred)\n",
    "        \n",
    "        l_auc.append(auc_roc)\n",
    "        l_grid.append(g)\n",
    "        print(f'{i}-Roc-AUC actual; {auc_roc}')\n",
    "        print(f'{i}-Con grid: {g}')\n",
    "        i+=1\n",
    "        # Guardamos si es mejor.\n",
    "        if auc_roc > best_score:\n",
    "            print('-'*10)\n",
    "            print(f'Mejor valor de ROC-AUC encontrado: {auc_roc}')\n",
    "            print(f'Grid{g}')\n",
    "            print('-'*10)\n",
    "            best_score = auc_roc\n",
    "            best_grid = g\n",
    "            best_estimator = clf_xgb\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    df_aux = pd.DataFrame({'auc': l_auc, 'grid': l_grid})\n",
    "    df_aux.to_csv('grid_search.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "me quedo con el mejor auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.89323\n",
      "Grilla: {'colsample_bytree': 0.9778268531494976, 'gamma': 0.39186326675646255, 'learning_rate': 0.02089730158577894, 'max_depth': 11, 'min_child_weight': 32.87831299488524, 'n_estimators': 1156, 'reg_lambda': 8.527804256249375, 'subsample': 0.7879622876290423}\n",
      "Tiempo transcurrido: 3318.050509929657 segundos\n",
      "Tiempo de entrenamiento por iteración: 41.48 segundos\n"
     ]
    }
   ],
   "source": [
    "if greedy: \n",
    "    print('ROC-AUC: %0.5f' % best_score)\n",
    "    print('Grilla:', best_grid)\n",
    "    print(f'Tiempo transcurrido: {str(end - start)} segundos')\n",
    "    print(f'Tiempo de entrenamiento por iteración: {str(round((end - start) / iterations, 2))} segundos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora con toda la data el prosible auc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC test: 0.89323\n"
     ]
    }
   ],
   "source": [
    "# y_pred = best_estimator.predict_proba(X_val)[:, 1]\n",
    "# auc_roc = sklearn.metrics.roc_auc_score(y_val, y_pred)\n",
    "# print('AUC-ROC test: %0.5f' % auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entreno con toda la data train+val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_grid = {'colsample_bytree': 0.7860370513913187, 'gamma': 4.070235476608438, 'learning_rate': 0.02486212527455788, 'max_depth': 11, 'min_child_weight': 14.046725484369038, 'n_estimators': 879, 'reg_lambda': 4.1399877303381505, 'subsample': 0.6481367528520412}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic', seed = random_state, eval_metric = 'auc', **best_grid,tree_method = 'gpu_hist') #enable_categorical = True\n",
    "clf_xgb.fit(X_train_all, y_train_all, verbose = False)\n",
    "final_model = clf_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_submit = final_model.predict_proba(eval_data.drop(columns=[ \"ROW_ID\"]))[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_preds = cls.predict_proba(eval_data.drop(columns=[\"ROW_ID\"]))[:, cls.classes_ == 1].squeeze()\n",
    "# Make the submission file\n",
    "submission_df = pd.DataFrame({\"ROW_ID\": eval_data[\"ROW_ID\"], \"conversion\": y_pred_submit})\n",
    "submission_df[\"ROW_ID\"] = submission_df[\"ROW_ID\"].astype(int)\n",
    "submission_df.to_csv(\"noche_testeo.csv\", sep=\",\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
